\newcommand{\subtype}{<:}

\chapter{Size Inference}
\label{chap:size-analysis}

A great many optimisations and safety checks in Futhark depend on how
the shape of two arrays relate to each other, or at which point the
shape of an array can be known (especially if that point is much
earlier than the point at which the \textit{values} of the array can
be known).  Especially the latter is important for the moderate
flattening transformation (\cref{chap:kernel-extraction}) that
constitutes one of the main contributions of the thesis.  Nested
parallelism supports the construction of arrays whose values are
dependent on some outer parallel construct.  However, for
\textit{regular} nested parallelism, the shapes of those arrays can be
computed invariant to all parallel loops.  It is crucial for efficient
execution that we can hoist the computation of such sizes out of
parallel loops.  This requires us to reify the notion of an array
shape, and the computation of that shape, in the IR.  In the Futhark
compiler, we treat size computations like any other expression, which
allows us to use our general compiler optimisation repertoire to
optimise and simplify the computation of sizes.

The main contribution of this chapter is an IR design that maintains
the invariant that for \textit{any} array-typed variable in scope,
each dimension of that array corresponds to some \lstinline{i32}-typed
variable also in scope.  For expressions where the shape of the result
cannot be computed in advance (consider a \kw{filter} or a function
call), we use a lightweight mechanism based on \textit{existential
  types}.  This chapter also discusses how we move from the unsized IR
to the sized IR (analogous to ``typed'' versus ``untyped''), and how
most size calculations can subsequently be optimised away.  An
important technique is \textit{function slicing}, which we use to
derive functions that precompute the size of values returned by a
function.

\Cref{sec:sized-ir} introduces the syntax of the IR, which is a slight
extension of the one presented in the previous chapter.  In
\cref{sec:size-analysis-intuition} we give an intuition for how size
inference functions, via an example of how a program progresses from
having no inferred sizes, to being highly existential, to finally
having most sizes resolved statically.  \Cref{sec:size-type-rules}
presents a subset of the type rules for the sized IR, and
\cref{sec:size-inference} shows some of the transformations used to
add size information to an unsized program.

\section{A Sized IR}
\label{sec:sized-ir}

One important observation of the IR presented in the previous chapter
is that some operator-semantics invariants, related to the array
regularity, are guaranteed to hold by construction, but several other
invariants are only ``assumed'', that is, they have not been verified
(made explicit) in the IR:
\begin{itemize}
\item \lstinline{iota} and \lstinline{replicate} assume a non-negative
  first argument, and the size of the resulting array is exactly the
  value of the first argument.
\item \lstinline{map} is guaranteed to receive arguments of identical
  outermost size, which also matches the outermost size of all result
  arrays\footnote{ In the user language \lstinline{zip} accepts an
    arbitrary number of array arguments that are required to have the
    same outermost size.}.  However, \lstinline{map} assumes that its
  function argument produces arrays of identical shape for each
  element of the input array.
\item \lstinline{filter} receives and produces arguments and results
  of identical outermost size, respectively (and the outermost size of
  the argument is not smaller than the one of the result).
\item \lstinline{reduce} and \lstinline{scan} receive arguments of
  identical outermost size, and \lstinline{scan} results have
  outermost size equal to that of the input.  The semantics for
  \lstinline{reduce} and \lstinline{scan} assumes that the two
  arguments and result of the binary associative operator have
  identical shapes.
\end{itemize}

\begin{figure}
\begin{tabular}{lrlr}
  $\tau$ & $::=$ & \mbox{\texttt{t} ~ $|$ ~ \texttt{[$d$]$\tau$}} & \mbox{(Scalar/array type)} \\
  $\ft$ & $::=$ & $(x_{1}: \tau_1) \rightarrow \cdots \rightarrow (x_{n}: \tau_{n}) \rightarrow \exists \bar{d}.\tty$ & (Sizes of results $\in \bar{d}$)\\
  $\etau$ & $::=$ & $\alpha~|~(x_{1}: \tau_{1}, \ldots, x_{n}: \tau_{n})~|~(x: \etau) \rightarrow \exists \bar{d}.\etau$ & ~~\mbox{(Sizes of results $\in \bar{d}$)} \\
  $\opty$ & $::=$ & $\forall\bar{\alpha}.\ft$ \\
  fun & ::= & \mbox{\Fun{$f$}{$\hat{p}$}{$\seq{d}.\utau$}{$e$}} ~~& ~~\mbox{(Sizes of results $\in \bar{d}$)} \\
\end{tabular}
\caption{Types with embedded size information and named parameters.
  The remaining syntax definitions remain unchanged, except that they
  refer to the new definitions above.}
\label{fig:sizeTypes}
\end{figure}

\begin{figure}
\begin{tabular}{lcl}
  \emph{op} & & \textrm{TySch}(\emph{op}) \\\hline
  {\lstinline!iota!} & : & $(d: \texttt{i32}) \rightarrow [d]\texttt{i32}$ \\
  {\lstinline!replicate!} & : & $\forall\alpha.(d: \texttt{i32}) \rightarrow \alpha \rightarrow [d]\alpha$ \\
  \lstinline[mathescape]!reshape! & : & $\forall\nseq{d}{m}\alpha.(x_{1}: \texttt{i32}, \ldots, x_{n}: \texttt{i32})$ \\
            & & ~~~~~~~~~~~~ $\rightarrow[d_{1}]\cdots[d_{m}]\alpha\rightarrow[x_{1}]\cdots[x_{n}]\alpha$ \\
  \lstinline[mathescape]!rearrange ($\nseq{c}{n}$)! & : & $\forall\nseq{d}{n}\alpha.[d_{1}]\cdots[d_{n}]\alpha\rightarrow[d_{p(1)}]\cdots[d_{p(n)}]\alpha$ \\
            & & ~~ \text{where $p(i)$ is the result of applying the}\\
            & & ~~ \text{permutation induced by $c_{1}, \ldots, c_{n}$}. \\
  {\lstinline!map!} & : & $\forall d\ \bar{\alpha}^{(n)}\bar{\beta}^{(m)}.$\\
            & & $~(\nseq{\alpha}{n} \rightarrow (\bar{\beta}^{(m)})) \rightarrow \nseq{[s]\alpha_i}{n} \rightarrow (\nseq{[s]\beta_i}{m})$\\
  {\lstinline!scatter!} & : & $\forall d \nseq{x}{m} \nseq{\alpha}{n}.$\\
            & & $~(\nseq{[x_{i}]\alpha_{i}}{n}) \rightarrow (\nseq{[d]\texttt{i32}}{n}) \rightarrow (\nseq{[d]\alpha_i}{n}) \rightarrow (\nseq{[x_{i}]\beta_i}{m})$\\
  {\lstinline!reduce!} & : & $\forall d \ \bar{\alpha}^{(n)}.$\\
  & & $~(\nseq{\alpha}{n} \rightarrow \nseq{\alpha}{n} \rightarrow (\bar{\beta}^{(n)})) \rightarrow (\nseq{\alpha}{n}) \rightarrow \nseq{[d]\alpha_i}{n} \rightarrow (\nseq{\alpha}{n})$ \\
  {\lstinline!scan!} & : & $\forall d \ \bar{\alpha}^{(n)}.$\\
  & & $(\nseq{\alpha}{n} \rightarrow \nseq{\alpha}{n} \rightarrow (\bar{\beta}^{(n)})) \rightarrow (\nseq{\alpha}{n}) \rightarrow \nseq{[d]\alpha_i}{n} \rightarrow (\nseq{[d]\alpha_i}{n})$ \\
  {\lstinline!filter!} & : & $\forall d_1 \ \bar{\alpha}^{(n)}.$\\
  & & $~(\nseq{\alpha}{n} \rightarrow \texttt{bool}) \rightarrow \nseq{[d_{1}]\alpha_i}{n} \rightarrow \exists d_{2}.(\nseq{[d_{2}]\alpha_i}{n})$ \\
  \kw{stream\_map} & : & $\forall d~\nseq{\alpha}{n}~\nseq{\beta}{m}.$\\
  & & $~((x: \texttt{i32})\rightarrow\nseq{[x]\alpha_{i}}{n} \rightarrow (\nseq{[x]\beta_{i}}{m})) \rightarrow \nseq{[d]\alpha_i}{n} \rightarrow (\nseq{[d]\beta}{m})$ \\
  \kw{stream\_red} & : & $\forall d~\nseq{\alpha}{n}\nseq{\beta}{m}.(\nseq{\beta}{m} \rightarrow \nseq{\beta}{m} \rightarrow (\nseq{\beta}{m})) $ \\
  & & $~~~\rightarrow ((x: \texttt{i32})\rightarrow\nseq{[x]\alpha_{i}}{n} \rightarrow (\nseq{\beta_{i}}{m}))$ \\
  & & $~~~\rightarrow \nseq{[d]\alpha_i}{n} \rightarrow (\nseq{\beta}{m})$ \\

\end{tabular}
\caption{Dependent-size types for various SOACs.}
\label{fig:soacSizeType}
\end{figure}

\Cref{fig:sizeTypes} shows an extended type system in which (i)
sizes are encoded in each array type, that is, $[d]\tau$ represents
the type of an array in which the outermost dimension has size $d$,
and in which (ii) function/lambda types use universal quantifiers for
the sizes of the array parameters ($\forall s_1$), and existential
quantifiers for the sizes of the result arrays ($\exists s_2$).
Function types now also contain \textit{named} parameters, supporting
a simple variant of dependent types.  For function parameters where
the name is irrelevant, we shall elide the name and use the same
notation as previously.
%
\Cref{fig:soacSizeType} also shows that this extension allows to
encode most of the afore-mentioned invariants into size-dependent
types.  The requirement for non-negative input to \lstinline{iota} and
\lstinline{replicate} remains a dynamic property.  We see that most
parameters remain unnamed, but are crucially used to encode the shape
properties of \lstinline{iota} and \lstinline{replicate}.

The type of \lstinline{map} is interesting because the result array
types do not follow immediately from the input array types.  Instead,
it is expected that the functional argument declares the result type
(including sizes) in advance.  Operationally, we can see this as being
able to ``pre-allocate'' space for the result.  However, the return
size cannot in general be known in advance without evaluating the
function.

The main difference between the size-dependent typing of the core
language, and the size annotations of the source language, is that the
latter are optional and checked dynamically, while the former are
mandatory and enforced statically.  As shown on
\cref{fig:soacSizeType}, the \kw{reshape} construct functions as an
``escape hatch'', by which we can arbitrarily transform the shape of
an array.  An implicit run-time check enforces that the original shape
and the desired shape has the same total number of elements.  This is
used to handle cases that would otherwise not be well-typed.

\section{Size Inference by Example}
\label{sec:size-analysis-intuition}

This section demonstrates, by example, the code transformation that
(i) makes explicit in the code the shape-dependent types and verifies
the assumed invariants and (ii) optimizes away in many cases the
existential types.  Our philosophy is to initially use existential
types liberally, with the expectation that inlining and simplification
will eventually remove almost all of them.

\begin{figure}
\begin{lstlisting}
let concat (xs: []f64) (ys: []f64): []f64 =
  let a = size 0 xs
  let b = size 0 ys
  let c = a+b
  let (is: []i32) = iota c
  let (zs: []f64) = map (\i -> if i < a then xs[i] else ys[i+b])
                        is
  in zs

let f (vss: [][]f64): []f64 =
  let (ys: []f64) (zs: []f64) =
    map (\(vs: []f64) ->
          let ys = reduce (+) 0.0 vs
          let zs = reduce (*) 1.0 vs
          in (ys, zs))
        vss
  let (rs: []f64) = concat ys zs
  in rs

let main (vsss: [][][]f64): [][]f64 =
  let (rss: [][]f64) =
    map (\(vss: [][]f64): []f64 ->
           let rs = f vss
           in rs) vs
  in rss
\end{lstlisting}

\caption{Running example: Program in un-sized IR.} 
\label{fig:RunEgSrc}
\end{figure}

The program in \Cref{fig:RunEgSrc} receives as input a
three-dimensional array \texttt{vsss}, and produces a two-dimensional
array, by mapping the elements of the outermost dimension of
\texttt{vsss} by function \texttt{f}.

\begin{figure}
\begin{lstlisting}
let concat @(n: i32) (m: i32)@ (xs: [@n@]f64) (ys: [@m@]f64)
           : @d.@[@d@]f64 =
  let a = @n@
  let b = @m@
  let c = a+b
  let (is: [@c@]i32) = iota c
  let (zs: [@c@]f64) =
   map (\i -> if i < n then xs[i] else ys[i+a]) is
  in zs

let f @(m: i32) (k: i32)@ (vss: [@m@][@k@]f64): @d@.[@d@]f64 =
  let (ys: [@m@]f64) (zs: [@m@]f64) =
    map (\(vs: [@k@]f64) ->
          let ys = reduce (+) 0.0 vs
          let zs = reduce (*) 1.0 vs
          in (ys, zs))
        vss
  let @(l: i32)@ (rss: [@l@]f64) = concat @m m@ ys zs
  in rs

let main @(n: i32) (m: i32) (k: i32)@
         (vsss: [@n@][@m@][@k@]f64): @d1 d2.@[@d@][@d1@][@d2@]f64 =
@  let l = if n != 0@
@          then let (d: i32) (ws: [d]f64) = f n m vsss[0]@
@               in d@
@          else 0@
  let (rss: [@n@][@d@]f64) =
    map (\(vss: [@m@][@k@]f64): [@l@]f64 ->
           let @(d: i32)@ @(w: [d]f64)@ = f @m k@ vss
@           let vs = reshape l w@
           in vs)
        vsss
  in rss
\end{lstlisting}

  \caption{Running example:
    $\exists$-quantified target IR.  Changes compared to
    \Cref{fig:RunEgSrc} highlighted in red.}
\label{fig:RunEgTgt}
\end{figure}

The first stage, demonstrated in \Cref{fig:RunEgTgt}, transforms
the program into an unoptimised version in which (i) all arrays have
shape-dependent types, which may be existentially quantified, and (ii)
all ``assumed'' invariants are explicitly checked.
%
This is achieved by:
\begin{itemize}
\item Extending the function signatures to encompass also the shape
  information for each array argument.  For example, \lstinline{f}
  takes additional parameters \lstinline{m} and \lstinline{k} that
  specify the shape of array argument \lstinline{vss},

\item Representing function's array results via
  existentially-quantified shape-dependent types.  For example, the
  return type of \lstinline{f} is specified as
  \lstinline{d.[d]f64}, indicating an existential size \lstinline{d}.

\item Modifying \lstinline{let} patterns to also explicitly bind any
  existential sizes returned by the corresponding expression.  For
  example, the binding of the result of \lstinline{concat} now also
  includes a variable \lstinline{l}, representing the result size.

\item For the \lstinline{map} in \lstinline{main}, we need to make a
  ``guess'' at the size of the array being returned, which we store as
  the variable \lstinline{l}.  This guess is made by applying the
  \kw{map} function to \lstinline{vsss[0]}, which produces both a size
  and an array, from which we use just the size.  This corresponds to
  \textit{slicing} the \kw{map} function.  If \lstinline{vsss} is
  empty (that is, if \lstinline{n} is zero), the guess is zero.

  Since \lstinline{f} returns an existential result, and the lambda
  \textit{must} return an array of exactly type \lstinline{[l]f64}, we
  use a \lstinline{reshape} to obtain this desired type.  Since the
  \lstinline{reshape} fails if \lstinline{d != n}, this effectively
  ensures that the \lstinline{map} produces a regular array.

\item Since all arrays in scope also have variables in scope for
  describing their size, replace all uses of \lstinline{size} with
  references to those varuables.
\end{itemize}

It is important to note that this transformation preserves
asymptotically the number of operations of the original program.
However, it performs a significant amount of redundant computation.
To compute \lstinline{l}, we compute the entire result, only to throw
most of it away.  General-purpose optimisation techniques can be
employed to eliminate the overhead.  On
\Cref{fig:SimplifyFShape} we see the result of inlining all
functions, followed by straightforward simplification, dead code
removal, and hoisting of the computation of \lstinline{c}.  The result
is that all arrays constructed inside the \lstinline{map}s have a size
that can be computed before the \lstinline{map}s are entered.  From an
operational perspective, this lets us pre-allocate memory before
executing the \lstinline{map}s on a GPU.  This is essential for GPU
execution because dynamic allocation and assertions are typically not
well suited for accelerators, hence the shapes of the result and of
various intermediate arrays need to be computed (or at least
overestimated) and verified before the kernel is run.

\begin{figure}
\begin{lstlisting}
let main (n: i32) (m: i32) (k: i32)
         (vsss: [n][m][k]f64): d1 d2.[d][d1][d2]f64 =
  let l = if n != 0 then m+m else 0
  let c = m+m
  let (rss: [n][d]f64) =
    map (\(vss: [m][k]f64): [l]f64 ->
          let (ys: [m]f64) (zs: [m]f64) =
            map (\(vs: [k]f64) ->
                  let ys = reduce (+) 0.0 vs
                  let zs = reduce (*) 1.0 vs
                  in (ys, zs))
                vss
          let (is: [c]i32) = iota c
          let (zs: [c]f64) =
            map (\i -> if i < n then xs[i] else ys[i+a])
                is
          let rs = reshape l zs
          in rs)
        vsss
  in rss
\end{lstlisting}

  \caption{After inlining all functions and performing simple
    inlining, dead-code elimination, and simplification---no
    existential quantification left.}
\label{fig:SimplifyFShape}
\end{figure}

However, there is still a problem with the current form of the code.
The issue is that the compiler cannot statically see that
\lstinline{l==m}, and thus has to maintain the \lstinline{reshape} and
perform a dynamic safety check at run-time.  This is because the
computation of \lstinline{l} is hidden behind a branch.  The branch
was conservatively inserted because we could not be sure that the
value of \lstinline{vsss[0]} would not be used for computing the size
(size analysis is intraprocedural, and so we have no insight in the
definition of \lstinline{f}), but now it is a hindrance to further
simplification.  There are at least two possible solutions, both of
which are used by the present Futhark compiler:

\begin{enumerate}
\item Give the programmer the ability to annotate the original lambda
  (in the source language) with the return type, \textit{including}
  expected size.  This effectively lets the programmer make the guess
  for us, with no branch required.  The result is still checked by a
  \lstinline{reshape}, but in most cases the guess will be statically
  the same as the computed size, and the \lstinline{reshape} can thus
  be simplified away.
\item Somehow ``mark'' the branch as being a size computation.  Then,
  after inlining and simplification, we can recognise such branches,
  and simplify them to their ``true'' branch, if that branch contains
  only ``safe'' expressions, where a safe expression is one whose
  evaluation can never fail.  We have to wait until after inlining and
  simplification, as a function call can never be considered safe.

  This solution has the downside that it may affect whether an inner
  size of an empty array is zero or nonzero.
\end{enumerate}

In practise, the first solution is preferable in the vast majority of
cases, as it also serves as useful documentation of programmer intent
in the source program.

A third solution is to factor out the ``checking'' part of the
\lstinline{reshape} operation.  This approach is sketched on
\Cref{fig:SimplifyFShapeCert}.  Here, we use a hypothetical
\lstinline{assert} expression for computing a ``certificate'', on
which the \lstinline{reshape} expression itself is predicated.  To
enable the check to be hoisted safely out of the outer
\lstinline{map}, the condition also succeeds if the outer map contains
no iterations (\lstinline{n == 0}).  The Futhark compiler currently
makes only limited use of this technique, as the static equivalence of
sizes is a more powerful enabler of other optimisations.

One may observe that in the resulting code, the shape and regularity
of \texttt{rss} are computed and verified before the definition of
\texttt{rss}, respectively, and, most importantly, that the size
computation and assumed-invariant verification introduce negligible
overhead, i.e., $O(1)$ number of operations.

\begin{figure}
\begin{lstlisting}
let main (n: i32) (m: i32) (k: i32)
         (vsss: [n][m][k]f64): d1 d2.[d][d1][d2]f64 =
  let l = if n != 0 then m+m else 0
  let c = m+m
@  let cert = assert(n == 0 || l==c)@
  let (rss: [n][d]f64) =
    map (\(vss: [m][k]f64): [l]f64 ->
          let (ys: [m]f64) (zs: [m]f64) =
            map (\(vs: [k]f64) ->
                  let ys = reduce (+) 0.0 vs
                  let zs = reduce (*) 1.0 vs
                  in (ys, zs))
                vss
          let (is: [c]i32) = iota c
          let (zs: [c]f64) =
            map (\i -> if i < n then xs[i] else ys[i+a]) is
          let rs = reshape@<cert>@ l zs
          in rs)
        vsss
  in rss
\end{lstlisting}

  \caption{Separating the size-checking of a \lstinline{reshape} from
    the \lstinline{reshape} itself.}
\label{fig:SimplifyFShapeCert}
\end{figure}

Note that the code shown on \Cref{fig:RunEgTgt} is the only
\textit{required} step we have to perform.  Subsequent optimisation to
eliminate existential quantification could be done in any way that is
found desirable, perhaps via interprocedural analysis or slicing.
Previously, we experienced with a technique based on \textit{slicing},
where a function \lstinline{g} is divided into two functions
\lstinline{g_shape} and \lstinline{g_value}, the first of which
computes the sizes of all (top-level) arrays in the latter, including
the result.  An example is shown on \Cref{fig:FShapeSlice},
which contains a portion of the running example.  The
\lstinline{concat} function has been split into
\lstinline{concat_shape} and \lstinline{concat_value}. The call to
\lstinline{concat} has been likewise split.

In practise, sophisticated dead code removal may be necessary to
obtain efficient shape functions --- for example, we will need to
remove shape-invariant loops --- and thus our approach generally
requires the compiler to possess an effective simplification engine.

The Futhark compiler currently does not use this approach of splitting
functions.  Partly because of the risk for asymptotic slowdown in the
presence of recursion (which was supported at the time), and partly
because merely inlining plus simplification is easier to implement,
and performed equally well for our purposes.

\begin{figure}

\begin{lstlisting}
let concat_shape (n: i32) (m: i32)
                 (xs: [n]f64) (ys: [m]f64): i32 =
  n+m

let concat_value (n: i32) (m: i32) (c: i32)
                 (xs: [n]f64) (ys: [m]f64): [c]f64 =
  let (is: [c]i32) = iota c
  let (zs: [c]f64) =
    map (\i -> if i < n then xs[i] else ys[i+n]) is
  in zs

let f (m: i32) (k: i32) (vss: [m][k]f64): d.[d]f64 =
  let (ys: [m]f64) (zs: [m]f64) =
    map (\(vs: [k]f64) ->
          let ys = reduce (+) 0.0 vs
          let zs = reduce (*) 1.0 vs
          in (ys, zs))
        vss
  let (l: i32) = concat_shape m m c ys zs
  let (rss: [l]f64) = concat_value m m c ys zs
  in rs
\end{lstlisting}

  \caption{An example of applying the slicing approach to \lstinline{concat}.}
  \label{fig:FShapeSlice}
\end{figure}

\section{New Type Rules}
\label{sec:size-type-rules}

\begin{figure}
\sempart{Existential subtyping}{\exists\seq{x}.\rho' \subtype \exists\seq{y}.\rho}

\fracc{
}{
  \exists\nseq{x}{n}.\rho \subtype \exists\nseq{x}{n}.\rho
}

\fracc{
  \exists\nseq{z}{l}.\rho'' \subtype \exists\nseq{y}{m}.\rho' \sp \exists\nseq{y}{m}.\rho' \subtype \exists\nseq{x}{n}.\rho
}{
  \exists\nseq{z}{l}.\rho'' \subtype \exists\nseq{x}{n}.\rho
}

\fracc{
  \mbox{$S$ is bijective from $\nseq{x}{n}$ to $\nseq{y}{n}$} \\
  \mbox{None of $y$ used free in $\rho$.}
}{
  \exists\nseq{y}{n}.S(\rho') \subtype \exists\nseq{x}{n}.\rho
}

\fracc{
  \mbox{$y$ used free in $\rho'$}
}{
  \exists~y~\nseq{x}{n}.\rho \subtype \exists~\nseq{x}{n}.\rho
}

\caption{The subtyping relationship for existential types.  The four
  rules describe reflexivity, transitivity, renaming, and weakening.}
  \label{fig:ext-type-operations}
\end{figure}

\begin{figure}

  \sempart{Expressions}{\Gamma \vd e : \exists\seq{d}.\rho}

  \fracc{
    \Gamma \vd e : \exists\seq{x}.\rho \quad \exists\seq{y}.\rho' \subtype \exists\seq{x}.\rho
  }{
    \Gamma \vd e : \exists\seq{y}.\rho'
  }

  \fracc{
    \vdp p : (\nseq{d_i: \texttt{i32}}{l},\rho) \sp \Gamma \vd e_1 : \exists\nseq{d}{l}.\rho \sp
    \\
    \Gamma,p \vd e_2 : \exists\nseq{d^r}{k}.\rho'
    \\
    \mbox{No name bound by $p$ used in $\rho'$}
  }{
    \Gamma \vd \Let{p}{e_1}{e_2} : \exists\nseq{d^r}{k}.\rho'
  }

\fracc{
  \textrm{lookup}_{\textrm{fun}}(f) = \nseq{(p_i: \tau_i)}{n} \rightarrow \exists\nseq{d}{l}\nseq{\tau'}{m}
    \\
    S = \langle \nseq{p_i \mapsto x_i}{n} \rangle \quad \forall i \in \Set{1,\ldots,n}.\Gamma(x_i) = S(\tau_i)
}
{\Gamma \vd f~x_{1}~\ldots~x_n : \exists\nseq{d}{l}.S(\nseq{\tau'}{m})}

\fracc{
  \Gamma \vda a_i : \etau^p_i \sp i \in \Set{1, \ldots, n}\\
  \textrm{TySch}(\emph{op}) = \nseq{(p_i: \etau_i)}{n} \rightarrow \exists\nseq{d}{l}.\nseq{\tau'}{m} \\
  S = \langle \nseq{p_i \mapsto x_i}{n}~|~\textrm{where $x_{i}=a_{i}$} \rangle \quad \forall i \in \Set{1,\ldots,n}.\etau^p_i = S(\etau_i)}{
  \Gamma \vd \emph{op}~\nseq{a}{n} : \exists\nseq{d}{l}.S(\nseq{\tau'}{m})
}

\fracc{
  \Gamma(s) = \texttt{bool} \\
  \Gamma \vd e_1 : \exists \seq{x}.\rho \sp \Gamma \vd e_2 : \exists \seq{x}.\rho
}{
  \Gamma \vd \If{s}{e_1}{e_2} : \exists\seq{x}.\rho
}

\fracc{
  \vdp p : \exists\nseq{d}{n}.\rho \sp \Gamma \vd \exists\nseq{d}{n}.e_1 : \rho \sp \Gamma \vd \exists\nseq{d}{n}.e_3 : \rho \\
  \Gamma \vd e_2 : \texttt{i32}
}{
  \Gamma \vd \Loop{\nseq{(d_i: \texttt{i32})}{n}~p}{e_1}{x}{e_2}{e_3} : \exists\nseq{d}{n}.\rho
}

\caption{Size-aware typing rules for interesting expressions.  The
  subtyping relationship is defined on
  \cref{fig:ext-type-operations}.}
  \label{fig:sizeTypeRules}
\end{figure}

The addition of size-dependent types requires an extension of the
typing rules.  The main problem is the handling of an existential
context in the type of an expression.  First, we define a subtyping
relation on existential types.  The judgment and its associated rules
is shown on \cref{fig:ext-type-operations}.  Apart from the usual
rules for reflexivity and transitivity, we define alpha substitution
(it is valid to change the names bound by the existential context),
and \textit{weakening}.  In our contxt, weakening corresponds to
making the size of an array type less concrete.  For example, we can
weaken the type $[x]\texttt{i32}$ to $\exists x.[x]\texttt[i32]$ (we
would usually also rename $x$ to avoid confusion).  The need for
weakening arises in particular due to the typing rules for \kw{let}
and \kw{if}, as we shall see.

We extend the type judgment for expressions such that it now returns
an a type with an existentially quantified part (which may be empty).
The most interesting rules are shown on \cref{fig:sizeTypeRules} and
discussed below.  We ignore uniqueness attributes here, as they have
no influence on size-dependent typing.

The rule for \kw{let} requires that the names bound may not be present
in the type of the returned value.  As an example, consider an
expression \lstinline{replicate x v}.  Supposing $\texttt{v}: \tau$,
this expression has type $[x]\tau$ %
while \mbox{\lstinline{let x = y in replicate x v}} has type
$\exists d.[d]\tau$ (with $d$ picked arbitrarily).  It is crucial that
we are able to use weakening to loosen the type of the body of the
\kw{let}-binding, or else we could not type it.

It is also in the rule for \kw{let}-bindings that we bind the
existential sizes to concrete variables.  For example, if the return
type of the function contains $l$ existential sizes, then we require
that the pattern begins with $l$ names of type \texttt{i32}.
Operationally, these will be bound to the actual sizes of the value
returned by the function.  As a slight hack, we require that these
names match exactly the corresponding existential context.  We can
always use alpha substitution to ensure a match.  For example, given a
function
\[
  \texttt{f}: (\texttt{x}: \texttt{i32}) \rightarrow (\texttt{y}: \tau) \rightarrow \exists \texttt{n}.[\texttt{x}][\texttt{n}]\tau
\]
we can derive a typing judgment for the \kw{let}-binding
\[
\Let{(\texttt{m}: \texttt{i32}, \texttt{v}: [\texttt{a}][\texttt{m}]\tau)}{\texttt{f}~\texttt{a}~\texttt{b}}{e}
\]
if we assume that $\texttt{a}: \texttt{i32}$ and $e$ is well-typed,
but
\[
\Let{(\texttt{m}: \texttt{i32}, \texttt{v}: [\texttt{m}][\texttt{a}]\tau)}{\texttt{f}~\texttt{a}~\texttt{b}}{e}
\]
cannot be typed using the given rules (note the swapped dimensions in
the type of \texttt{v}).

Using weakening, a pattern can also contain existential parts that are
not immediately existential in the type.  This is useful for
supporting gradual simplification of existential sizes, without having
the intermediate steps be ill-typed For example, consider this
expression:

\[
\Let{\mbox{\lstinline{(d: i32) (a: [d]i32)}}}{\mbox{\lstinline{(let y = x in replicate y 0)}}}{e}
\]

Since the size \texttt{y} is bound inside of the outer
\kw{let}-binding, we have no choice but to leave the size \texttt{d}
existential.  If the compiler then performs copy-propagation, we
obtain the following expression:

\[
\Let{\mbox{\lstinline{(d: i32) (a: [d]i32)}}}{\mbox{\lstinline{replicate x 0}}}{e}
\]

The type of \lstinline{replicate x 0} is \lstinline{[x]i32}, but since
copy-propagation is not concerned with modifying patterns in
\kw{let}-patterns, we still existentially quantify the size of the
result.  A subsequent simplification can then be applied to remove
unnecessary existential parts of patterns, by noting that \texttt{d}
will always be \texttt{x}, yielding:

\[
\Let{\mbox{\lstinline{(a: [x]i32)}}}{\mbox{\lstinline{replicate x 0}}}{e}
\]

The vast majority of the existential-related simplification we perform
is of the trivial nature above, where gradual rewrites eventually
bring the program to a form where the existential sizes can be
statically resolved.

From a compiler engineering point of view, it would be awkward if
copy-propagation (or other optimisations) were also responsible for
fixing any changes to existential sizes that were caused by their
simplifications.  It is cleaner to separate this into multiple
separate transformations, but this requires that the intermediate
forms are well-typed.  This fits well with the general philosophy in
the Futhark compiler of gradual refinement, as was also shown in the
previous section.  This leniency will also help us when first
inserting the existential information, as shown in the next section.
However, making a type \textit{more} existential would still require
us to fix up the relevant patterns.

Another interesting case is for functions and operators, due to named
parameters.  We assume that all arguments to the function are variable
names.  Intuitively, we then construct a substitution $S_{p}$ from the
parameter names in the type of the function to the concrete names of
the arguments, and apply this substitution before checking whether the
argument types match the parameter types.  The substitution is also
applied to the result type.

We use a similar trick for \kw{if} expressions, where both branches
must return existential types with identical existential contexts.
For example, consider the following expression:
\[
  \If{c}{\kw{replicate}~x~(\kw{iota}~y)}{\kw{replicate}~y~(\kw{iota}~x)}
\]
The ``true'' branch has type $[x][y]\texttt{i32}$, while the ``false''
branch has type $[y][x]\texttt{i32}$.  The least existential type that
can encompass both of these is $\exists n m.[n][m]\texttt{i32}$, which
must thus be the type of the \kw{if}-expression.

\section{Size Inference}
\label{sec:size-inference}

\newcommand{\transformBnd}[1]{\mathcal{A}_{#1}^{\text{exp}}}
\newcommand{\transformFun}{\mathcal{A}^{\text{fun}}}
\newcommand{\transformLam}[1]{\mathcal{A}_{#1}^{\text{lam}}}
\newcommand{\head}{\textrm{head}}
\newcommand{\tail}{\textrm{tail}}
\newcommand{\drop}{\textrm{drop}}
\newcommand{\cons}[2]{#1 :: #2}

This section presents a set of syntax-directed rules for transforming
an un-annotated Futhark program into an \textit{annotated} Futhark
program, where all types have size information.  Concretely, we are
performing a syntax-directed translation that carries out the
following tasks:

\begin{enumerate}
\item Add existential contexts to function return types to match the
  number of (distinct) array dimensions.
\item Add extra parameters to all functions corresponding to the
  number of (distinct) array dimensions in their parameters.
\item Add existential contexts to all \kw{let}-patterns corresponding
  to the existential type of the bound expression.
\item Insert \kw{reshape} expressions where necessary (e.g. for inputs
  to \kw{map}) to ensure that size-related restrictions are
  maintained.
\item Amend any type $[]\tau$ such that it has form $[d]\tau$, where
  $d$ is some variable in scope.  After the preceding steps have been
  accomplished, this can be done by looking at the context in which
  the type occurs and following the type rules.
\end{enumerate}

We will ignore the distinction between unique and non-unique types, as
these have no influence on size inference.  We write $\text{d}(\tau)$
to indicate the \emph{rank} (number of dimensions) of a type $\tau$.
We may also write $\text{d}(v)$, where $v$ is not itself a type, but
something which {\em has} a type, such as a variable.

\subsection{Fundamental Transformation}
\label{sec:FundamentalTransformation}

\newcommand{\ext}{\textrm{ext}}
\newcommand{\shape}{\textrm{shape}}
\newcommand{\val}{\textrm{value}}
\newcommand{\checking}{\textrm{checking}}

For each function $f$ in the original program, we generate the
\textit{existential function} $f_{\ext}$, that returns the values
returned by $f$, with all shapes in the return type being
existentially quantified.

Specifically, if the return type of a function \(f\) is
$\nseq{\tau}{n}$, then the return type of \(f_{\ext}\) is
\(\exists\nseq{d^{1}_{i}}{\text{d}(\tau_{1})}\cdots\nseq{d^{n}_{i}}{\text{d}(\tau_{n})}.(\tau_{1}',
\ldots, \tau_{n}')\), where $\text{d}(\tau)$ is the rank of $\tau$,
and \(\tau_{j}'\) is \(\tau_{j}\) shape-annotated with
\(\nseq{d^{i}_{i}}{\text{d}(\tau_{j})}\).
%
For example, if \(f\) returns type \([][]\texttt{i32}\), \(f_{\ext}\)
will return type \(\exists d_{1}~d_{2}[d_{1}][d_{2}]\texttt{i32}\).
Thus, after transformation, the shape of the return of a function will
be existentially quantified.

Furthermore, the parameters of \(f\) are likewise annotated.  An
explicit \texttt{i32} parameter is added for every dimension of an
array parameter, with the array parameter itself annotated to refer to
the corresponding \texttt{i32} parameter.  For example, if \(f\) takes
a single parameter \([][]\texttt{i32} p\), then \(f_{\ext}\) will take
three parameters \(\texttt{i32}~n\), \(\texttt{i32}~m\), and
\([m][n]\texttt{i32}\)

In this chapter we assume for simplicity that the original program
contains no pre-existing size information, as by the un-sized IR
presented in the previous chapter.  This is not congruent with the
actual Futhark source language, where user-provided shape invariants
may be present.  In the implementation, these are handled by imposing
constraints on the dimensions we generate.  For example, if the source
program contains information relating certain dimensions of the
function return type to specific function parameters, we do not
generate existential dimensions, but instead refer directly to the
function parameters.  Likewise, if the source program specifies that
certain dimensions are identical to each other, we simply generate a
single size parameter and use that for all the dimensions in question.

\begin{figure}[bt]

\begin{align*}
  \mathcal{T}(\texttt{[]}_{1}\cdots\texttt{[]}_n\texttt{t},\nseq{s}{n}) = \texttt{[$s_{1}$]$\cdots$[$s_{n}$]\texttt{t}}
\end{align*}%
\caption{Annotating types with shapes.}
\label{fig:annotating bindings}
\end{figure}

We will use function \(\mathcal{T}(\tau~x,s)\) in
Figure~\ref{fig:annotating bindings} to annotate a type \(\tau\) with
the shapes in \(s\), which is a sequence of variable names whose
length is equal to the rank of \(\tau\).  As an example,
\[
\mathcal{T}(\texttt{[][]i32}, \texttt{n}, \texttt{m}) = \texttt{[m][n]i32}
\]

\subsection{Transformation Functions}
\label{sec:TransformationFunctions}

The function \(\transformBnd{\Sigma}(b)\) computes the \textit{annotated}
version of the body \(b\) in the environment \(\Sigma\) and returns
shapes as well as values (that is, its type will contain
existentials).  The environment \(\Sigma\) is a mapping from names of
arrays to lists of variable names, where element \(i\) of the list
denotes the size of dimension \(i\) of the corresponding array.  We
will use conventional \(\head\), \(\tail\) and \(\drop\) operations to
manipulate these lists, as well as bracket notation for arbitrary
indexing; we write the ``cons'' operation as \(\cons{x}{xs}\).

The function \(\transformFun(f)\) computes the existentially-quantified 
function \(f_{\ext}\), by using  \(\transformBnd{\Sigma}\) to annotate
the function's body (and result) with shapes information, and by modifying
the function's type as described in the beginning of
Section~\ref{sec:FundamentalTransformation}.

We also have a function
\(\transformLam{\Sigma}(\lambda,\seq{r},\seq{p})\).  This is
similar to the function transformation, except that (i) we work within
an environment \(\Sigma\), and (ii) we know in advance the intended shape
of the result (\(\seq{r}\)) and parameters
(\(\seq{p}\)), because the result shape of an anonymous function
is never existentially quantified.

\subsection{Simple Rules}
\label{sec:SimpleRules}

This section describes cases for the function
\(\transformBnd{\Sigma}(e)\).  The core rules are listed in
Figure~\ref{fig:simple-transformation-rules}, including the rule for
\kw{let}-patterns, which is likely the most important one.  Here we
create an existential size for \textit{every} dimension of the bound
variables.  It is assumed that subsequent simplification will remove
those that can be resolved statically.

Observe how \texttt{size} expressions are completely removed from the
annotated program, and instead replaced with the variable storing the
desired size.  The rule for \kw{if} is completely straightforward, and
simply applies the transformation to the subexpressions.  Most of the
rules are of this form, and have been elided.  A call to a function
\(f\) becomes a call to the function \(f_{\ext}\), where argument
sizes are passed explicitly.

\begin{figure}

\[
\begin{array}{l}
  \transformBnd{\Sigma}{\Let{\nseq{v_{i}: \tau_{i}}{n}}{e_{1}}{e_{2}}} = \\
  \quad \Let{\nseq{\textrm{sizes}(\tau_{i})}{n}}{\transformBnd{\Sigma}{e_{1}}}{\transformBnd{\Sigma'}{e_{2}}} \\
  ~~ \textrm{where}
  \begin{array}[t]{ll}
    \textrm{sizes}(\tau_j) &= \nseq{d^{j}_{i}}{\textrm{d}(\tau_{j})} \\
    \Sigma' &=\nseq{v_{i}\mapsto\textrm{sizes}(\tau_{i})}{n},\Sigma
  \end{array}
  \\
  \\
\transformBnd{\Sigma}(\Let{(v: \texttt{i32})}{\kw{size}~k~a}{e}) = \\
\quad \Let{(v: \texttt{i32})}{\Sigma(a)[k]}{\transformBnd{\Sigma}(e)} \\
\\
\\
\transformBnd{\Sigma}(\If{e_{1}}{e_{2}}{e_{3}}) = \\
\quad \If{\transformBnd{\Sigma}(e_{1})}{\transformBnd{\Sigma}(e_{2})}{\transformBnd{\Sigma}(e_{3})}
  \\
  \\
  \transformBnd{\Sigma}(f~\nseq{x}{n}) = \\
  \quad f_{ext}~\nseq{\Sigma(x_{i})}{n}~\nseq{x}{n}
\end{array}
\]

\caption{Simple transformation rules for expressions.}
\label{fig:simple-transformation-rules}
\end{figure}

\subsection{SOAC-related Rules}
\label{sec:SOACSizeRules}

There are two issues to be dealt with when size-transforming SOACs.
The first is that the type rules indicate that the outer dimension of
all input arrays must be identical.  We deal with this by using
\kw{reshape} to transform all input arrays to have the same outermost
size as the first input array.

The second, and bigger problem with annotating SOACs is that, in
Futhark, anonymous functions cannot be existentially quantified.
Hence, before evaluating the SOAC, we must know the shape of its
return value.  For this section, we assume that given an anonymous
function \(\lambda\), it is possible to compute \(\lambda_{\shape}\),
which returns the shape of the values that would normally be returned
by \(\lambda\).

Similarly to function calls, when transforming $\kw{map}~\lambda~a$,
there are two possible avenues of attack.

\begin{description}
\item[Pre-assertion] First, calculate $\kw{map}~\lambda_{\shape}~a$, and
  assert, for each returned array, that all of its elements are
  identical. That is, check in advance that the \kw{map} expression
  results in a regular array.  After this check, we know with
  certainty the shape of the values returned by the original map
  computation (and that it will be regular), which we can then use to
  annotate the \kw{map} computing the values.  This strategy would
  require an extension to support an explicit \kw{assert} expression.

\item[Intra-assertion] Alternatively, we can compute
  \(\lambda_{\shape}~a[0]\), the shape of the first element of the
  result.  Then, we modify $\lambda$ to explicitly \kw{reshape} its
  result to the same shape as that computed for the first element - we
  call this modified version \(\lambda_{\textrm{checking}}\).  This is
  the approach we used on \cref{fig:RunEgTgt}, and is illustrated on
  \cref{fig:sizeTransformMap}.  For brevity we have omitted the
  \kw{if}-branch guarding the case when the input array is empty.
\end{description}

The former approach is only efficient if \(\lambda_{\text{\shape}}\)
is efficient compared to \(\lambda\) - in practice, it must not
contain any loops.  The latter approach is limited in that it forces
shape checking inside the value computation, which means that we do
not know in advance whether the shape annotation is correct.  However,
in practice, the \kw{reshape} operations often end up being statically
simplified away.  Hence, our compiler always applies the
intra-assertion rule, with the expectation that a later optimisation
will remove the assertions, if possible.

\begin{figure}[t]
  \[
\begin{array}{l}
  \transformBnd{\Sigma}(\kw{map}~\lambda~\nseq{a}{n})) = \\
  \quad \kw{let}~s_{1}^{2} \ldots s_{1}^{\text{d}(\tau_{1})} \ldots s_{m}^{2}, \ldots, s_{m}^{\text{d}(\tau_{m})}~=~\lambda_{\text{\shape}}~\nseq{a_{i}\texttt{[0]}}{n} \\
  \quad \kw{in}~\kw{map}~\lambda_{\text{checking}}~\nseq{\textrm{shapeup}(a_{i})}{n} \\
  \text{where}
  \begin{array}[t]{ll}
    (\nseq{\tau}{m}) & = \mbox{Return type of $\lambda$} \\
    \textrm{shapeup}(a_{i}) & = \kw{reshape}~(\head(\Sigma(a_{1})) :: \tail(\Sigma(a_{i})))~a_{i} \\
    r_{i} & = \cons{s_{i}^{2}}{\cons{\ldots}{s_{i}^{\text{d}(\tau_{i})}}} \\
    p_{i} & = \tail(\Sigma(a_{i})) \\
    \lambda_{\text{\checking}} & = \transformLam{\Sigma}(\lambda,\nseq{r}{m},\nseq{p}{n}). \\

  \end{array}
\end{array}
\]

\caption{Transformation for \kw{map} using the intra-assertion strategy.  For brevity we have elided the branch guarding against the case where the input arrays are empty.}
\label{fig:sizeTransformMap}
\end{figure}

Similar approaches are used for the remaining SOACs.  The cases for
\kw{reduce}, \kw{scan}, \kw{scatter}, and \kw{stream\_red} are simple
because the correct return shape for the anonymous functions can be
deduced from the inputs to the SOAC.  The case for \kw{stream\_map} is
simple because the return shape must match the chunk size.  These
cases are all handled by inserting \kw{reshape} expressions in the
functions.

\section{Related Work}

An important piece of related work is the work on the FISh \cite{fish}
programming language, which uses partial evaluation and program
specialization for resolving shape information at compile time.  The
semantics of FISh guarantees that this is possible.  A similar
approach is used in \~{F}~\cite{shaikhha2017using}, with the specific
motivation of being able to efficiently pre-allocate memory.

Futhark uses approximately the same strategy, but adds existential
types to handle constructs such as \kw{filter}, at the cost of no
longer being able to fully resolve shape information statically in all
cases.

Much work has also gone into investigating expressive type systems,
based on dependent types, which allow for expressing more accurately,
the assumptions of higher-order operators for array operations
\cite{AgdaAccellerate,Trojahner:2008,Trojahner2009643}. Compared to
the present work, such type systems may give the programmer certainty
about particular execution strategies implemented by a backend
compiler. The expressiveness, however, comes at a price. Advanced
dependent type systems are often very difficult to program and
modularity and reusability of library routines require the end
programmer to grasp, in detail, the underlying, often complicated,
type system.  Computer algebra systems~\cite{AXIOM,AldorCAH} have also
provided for a long time a compelling application of dependent types
in order to express accurately the rich mathematical structure of
applications, but inter-operating across such systems remains a
significant challenge~\cite{mapal_synasc,alma:ISSAC}.

A somewhat orthogonal approach has been to extend the language
operators such that size and bounds checking invariants always
hold~\cite{ElsmanDybdal:Array:2014}, the downfall being that
non-affine indexing might appear.  The Futhark strategy is instead to
rely on advanced program analysis and compilation techniques to
implement a pay-as-you-go scheme for programming massively parallel
architectures.

Another strand of related work is the work on effect systems for
region-based memory management \cite{mlkit_retrospective}, in
particular, the work on multiplicity inference and region
representation analysis in terms of physical-size inference
\cite{vejlstrup94,btv96}. Whereas the goal of multiplicity inference
is to determine an upper bound to the number of objects stored into a
region at runtime, physical-size inference seeks to compute an upper
bound to the number of bytes stored in a single write. Compared to the
present work, multiplicity inference and physical-size inference are
engineered to work well for common objects such as pairs and closures,
but the techniques work less well with objects whose sizes are
determined dynamically.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
