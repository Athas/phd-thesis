\renewcommand{\G}[1]{G#1}

\chapter{Moderate Flattening and Kernel Extraction}
\label{chap:kernel-extraction}

This chapter presents a transformation that aims to enhance the degree
of statically-exploitable parallelism by reorganizing imperfectly
nested parallelism into perfect SOAC nests.  The outer levels of the
resulting nestings correspond to \kw{map} operators, which are trivial
to map to GPUs, and the innermost one is an arbitrary SOAC or
sequential code.  In essence, the transformation seeks to rewrite the
program to express patterns of parallelism that have a known efficient
implementation to GPUs.  The simplest such pattern is a perfect nest
of \kw{map}s, where the innermost function contains arbitrary
sequential code.  This pattern maps directly to a single GPU kernel,
but there are other interesting patterns that correspond to, for
example, segmented scans and reductions.

In a purely functional setting, Blelloch's
transformation~\cite{blelloch1990vector} flattens all available
parallelism, while asymptotically preserving the depth and work of the
original nested-parallel program.  In our setting, this corresponds to
interchanging all sequential loops outwards, and forming
\kw{map}-nests that contain only simple scalar code.  While
asymptotically efficient, the approach is arguably inefficient in
practice~\cite{bergstrom2012nested}, for example because it does not
account for locality of reference, and pays a potentially large cost
in memory usage to extract parallelism that may not be necessary.  For
example, a fully flattened implementation of matrix multiplication
requires $O(n^{3})$ storage, where the usual implementation uses only
$O(n^{2})$~\cite{Spoonhower:2008:SPP:1411204.1411240}.

Our algorithm, presented below, builds on \kw{map}-\kw{loop}
interchange and \kw{map} distribution, exploiting the property that it
is always safe to interchange a parallel loop inwards, or to
distribute a parallel loop~\cite{Allen-Kennedy2002}.  Our algorithm
attempts to exploit some of the efficient \textit{top-level
  parallelism}.  Two important limitations are:

\begin{enumerate}
\item We do not exploit parallelism inside \kw{if} branches, as this
  generally requires expensive \kw{filter} operations.
\item We terminate distribution when it would introduce irregular
  arrays, as these obscure access patterns and prevent further
  spatial- and temporal-locality optimizations.
\end{enumerate}

Our algorithm is less general than full flattening, but generates more
analysable code in the common case, for programs that do not require
the generality of flattening.  Also, our presented algorithm is not
\textit{cost-preserving}, in the notion of
Blelloch~\cite{Blelloch:1996:PTS:232627.232650}.  Should we assign a
NESL-style cost model to Futhark, the result of our moderate
flattening algorithm would often have a different asymptotic cost
(less parallelism) than the original program.

On the upside, moderate flattening enables the optimisations covered
in \cref{chap:tiling}.  Multi-versioned code, discussed in
\cref{sec:multi-versioned-code}, could be used to apply full
flattening as a fallback for those cases where moderate flattening is
incapable of extracting sufficient parallelism.  However, this
technique is not yet implemented.

\section{Example of Moderate Flattening}
\label{sec:kernel-extraction}

\begin{figure}
\begin{subfigure}[b]{0.48\columnwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily,numbers=none,xleftmargin=0pt,moredelim={[is][\color{blue}\bfseries]{@}{@}}]
let (asss, bss) =
 @map@
  (\ps: ([m][m]i32,[m]i32) ->
   let ass =
    map
     (\p: [m]i32 ->
      let cs =
        scan (+) 0 (iota p)
      let r = reduce (+) 0 cs
      let as = map (+r) ps
      in as) ps
   let bs =
    loop (ws=ps) for i < n do
     let ws' =
      map
       (\as w: i32 ->
        let d = reduce (+) 0 as
        let e = d + w
        let w' = 2 * e
        in w') ass ws
     in ws'
   in (ass, bs))
  pss
\end{lstlisting}
\caption{Program before distribution.}
\label{fig:before-distrib}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.49\columnwidth}
\begin{lstlisting}[basicstyle=\scriptsize\ttfamily,numbers=none,xleftmargin=0pt,moredelim={[is][\color{blue}\bfseries]{@}{@}}]
let rss =
 @map@
  (\ps: [m]i32 ->
    @map@
     (\p: i32 ->
      let cs = scan (+) 0 (iota p)
      let r = reduce (+) 0 cs
      in r) ps) pss
let asss =
 @map@ (\ps rs: [m]i32 ->
       @map@ (\r ->
             @map@ (+r) ps)
          rs)
     pss rss
let bss =
 loop (wss=pss) for i < n do
  let dss =
   @map@ (\ass: [m]i32 ->
         @map@ (\as: i32 ->
               @reduce@ (+) 0 as)
             ass)
        asss
  in @map@ (\ws, ds: [m]i32 ->
           @map@ (\w d: i32 ->
                 let e = d + w
                 let w' = 2 * e
                 in w')
               ws ds)
         wss dss
\end{lstlisting}
\caption{Program after distribution.}
\label{fig:after-result}
\end{subfigure}
\caption{Extracting kernels from a complicated nesting.  We assume
  $\texttt{pss} : [m][m]\texttt{i32}$.  Exploitable (outermost or perfectly
  nested) levels of parallelism are highlighted in blue.}
\end{figure}

Figures~\ref{fig:before-distrib}~and~\ref{fig:after-result}
demonstrate the application of our algorithm on a contrived but
illustrative example that demonstrates many of the flattening rules
exploited in the generation of efficient code for the various
benchmark programs.  The original program consists of an outer
\lstinline{map} that encloses (i) another \lstinline{map} operator
implemented as a sequence of \lstinline{map}s, \lstinline{reduce}s,
and \lstinline{scan}s and (ii) a loop containing a \lstinline{map}
whose implementation is given by a \lstinline{reduce} and some scalar
computation.  As written, only one level of parallelism (for example,
the outermost) can be statically mapped on GPGPU hardware. Our
algorithm distributes the outer \lstinline{map} across the enclosed
\lstinline{map} and \lstinline{loop} bindings, performs a
\lstinline{map}-\lstinline{loop} interchange, and continues
distribution.  The result consists of four perfect nests: a
\lstinline{map-map} and \lstinline{map-map-map} nest at the outer
level, and a \lstinline{map-map-reduce} (segmented reduction) and
\lstinline{map-map} nest contained inside the loop.  In the first
\lstinline{map-map} nest, the \lstinline{scan} and \lstinline{reduce}
are sequentialized because further distribution would generate an
irregular array, as the size \lstinline{p} of \lstinline{cs} is
variant to the second \lstinline{map}.

\section{Rules for Moderate Flattening}
\label{sec:moderate-flattening-rules}

\begin{figure}
\sempart{Basic Flattening Rules}{\Sigma \vd e \Rightarrow e}

\begin{align*}
\inference{
\Sigma \vd \Map~\Par{\fn~\seq{x}~\rightarrow~e}~\seq{y} \Rightarrow e'
}{
\Sigma, \M~\seq{x}~\seq{y} \vd e \Rightarrow e'
}
\tagsc{G1}
\end{align*}

\begin{align*}
\inference{
\Sigma, \M~\seq{x}~\seq{y} \vd e \Rightarrow e'
}{
\Sigma \vd \Map~\Par{\fn~\seq{x}~\rightarrow~e}~\seq{y} \Rightarrow e'
}
\tagsc{G2}
\end{align*}

\begin{align*}
\inference{
}{
\emptyset \vd e \Rightarrow e
}
\tagsc{G3}
\end{align*}

\begin{align*}
\inference{
  \Sigma = \Mv{x_p}{y_p},\ldots,\Mv{x_1}{y_1} \\
  \Sigma' = \M~(\seq{x_p},\seq{a_{p-1}})~(\seq{y_p}, \seq{a_{p}}), \ldots, \M~(\seq{x_1},\seq{a_{0}})~(\seq{y_1},\seq{a_{1}}) \\
  \seq{a_{p}}, \dots, \seq{a_{1}} ~\mbox{fresh~names} \\
  \mbox{size~of~each~array~in~}\seq{a_{0}}\mbox{~invariant~to~}\Sigma \\
  \Sigma \vd e_1 \Rightarrow e_1' ~~~~ \Sigma' \vd e_2 \Rightarrow e_2'
}{
\Sigma \vd \Let{\seq{a_0}}{e_1}{e_2} \Rightarrow \Let{\seq{a_p}}{e_1'}{e_2'}
}
\tagsc{G4}
\end{align*}

\begin{align*}
\inference{
  g = \Reduce~\Par{\fn~\seq{y}^{2*p} ~\rightarrow ~e}~\seq{n}^p \\
  \Sigma \vd \Map~\Par{g} ~\Par{\Transpose~z_0}\ldots\Par{\Transpose~z_{p-1}} \Rightarrow e' \\
  f = \Map~\Par{\fn~\seq{y}^{2*p}~\rightarrow e}
}{
\Sigma \vd \Reduce~\Par{f}~\Par{\seq{\Replicate~k ~n}^p}~\seq{z}^p \Rightarrow e'
}
\tagsc{G5}
\end{align*}

\begin{align*}
\inference{
\Sigma \vd \Rearrange~\Par{0,1+k_0,\ldots,1+k_{n-1}}~y \Rightarrow e
}{
\Sigma, \M~x~y \vd \Rearrange~\nseq{k}{n} ~ x \Rightarrow e
}
\tagsc{G6}
\end{align*}

\begin{align*}
\inference{
  \Sigma' = \Sigma, \M~(\seq{x},\seq{y})~(\seq{xs},\seq{ys}) \hspace{1.2cm} (\{n\}\cup\seq{q}) \cap (\seq{x},\seq{y}) = \emptyset \\
  m = \mbox{outer~size~of~each~of~} \seq{xs} \mbox{~and~} \seq{ys}\\
  \mbox{f~contains~exploitable~(regular)~inner~parallelism}\\
  \Sigma \vd \Loop~\Par{\seq{zs'}~\kt{=}~\seq{\Replicate~m~z_i}, \seq{ys'}~\kt{=}~\seq{ys}} \\
    \hspace{1.8cm}\For~ i~\kt{<}~n~ \Do~\Map \Par{f~i~\seq{q}} ~\seq{xs}~\seq{ys}~\seq{ys'}~\seq{zs'} \Rightarrow e
}{
  \Sigma' \vd \begin{array}[t]{l} \Loop~\Par{\seq{z'}~\kt{=}~\seq{z},~\seq{y'}~\kt{=}~\seq{y}}~\\
  ~~ \For~i~\kt{<}~n~ \Do~ f~i~\seq{q}~\seq{x}~\seq{y}~\seq{y'}~\seq{z} \Rightarrow e \end{array}
}
\tagsc{G7}
\end{align*}

\caption{The flattening rules that form the basis of the moderate flattening algorithm.}
\label{fig:basic-flattening-rules}
\end{figure}

Figure~\ref{fig:basic-flattening-rules} lists the rules that form the basis
of the flattening algorithm.
%
We shall use $\Sigma$ to denote \emph{map nest contexts},
which are sequences of \emph{map contexts}, written $\Mv{x}{y}$, where
$\ov{x}$ denotes the bound variables of the map operator over the
arrays held in $\ov{y}$.
%Map xs ys e = forall xs in ys do e
The flattening rules, which take the form $\Sigma \vd e \Rightarrow
e'$, specify how a source expression $e$ may be translated into an
equivalent target expression $e'$ in the given map nest context
$\Sigma$. Several rules may be applied in each situation. The
particular algorithm used by Futhark bases its decisions
% base decisions about whether a rule should be applied
on crude heuristics related to the structure of the map nest context
and the inner expression.  Presently, nested \lstinline{stream_par}s are
sequentialised, while nested \lstinline{map}s, \lstinline{scan}s, and
\lstinline{reduce}s are parallelised.  These rules were mainly chosen
to exercise the code generator, but sequentialising \lstinline{stream_par} is
the right thing to do for most of the data sets we use in
Section~\ref{chap:empirical-validation}.

For transforming the program, the flattening algorithm
is applied (in the empty map nest context) on each map nest in
the program.
%
Rule~\G{1} (together with rule~\G{3}) allows for manifestation of the
map nest context $\Sigma$ over $e$. Whereas rule~\G{1} can be applied
for any $e$, the algorithm makes use of this rule only when no other
rules apply.
%
Given a map nest context $\Sigma$ and an instance of a \texttt{map}
SOAC, rule~\G{2} captures the \texttt{map} SOAC in the
map nest context. This rule is the only rule that extends the map
nest context.

%Notice, in
%particular, that the rule for map fission has a side condition that
%ensures regularity of intermediate arrays.

Rule~\G{4} allows for map fission
($\map~(f\circ g) \Rightarrow \map~f\circ \map~g$), in the sense that
the map nest context can be materialized first over $e_1$ and then
over $e_2$ with appropriate additional context to allow for access to
the now array-materialized values that were previously referenced
through the let-bound variables $\overline{a_0}$. The rule can be
applied only if the intermediate arrays formed by the transformation
are ensured to be regular, which is enforced by a side condition in
the rule. To avoid unnecessary and excessive flattening on scalar
computations, the \lstinline{let}-expressions are rearranged using a
combination of \lstinline{let}-floating
\cite{PeytonJones:1996:LMB:232627.232630} and tupling for grouping
together scalar code in a single \lstinline{let}-construct.  In
essence, inner SOACs are natural splitting points for fission.  For
example,
\begin{lstlisting}[aboveskip=-0.6\baselineskip]
let b = x+1
let a = b+2
in replicate n a
\end{lstlisting}
is conceptually split as
\begin{lstlisting}[aboveskip=-0.5\baselineskip]
let a = (let b = x+1 in b+2)
in replicate n a
\end{lstlisting}

Rule~\G{5} allows for \kw{reduce}-\kw{map} interchange where it is
assumed that the source neutral reduction element is a replicated
value.  The original pattern appears in $k$-means (see
\cref{fig:stream-means}) as a reduction with an array operator, which
is inefficient on a GPU if executed as such. The interchange results
in a segmented-reduce operator (applied on equally-sized segments), at
the expense of transposing the input array(s).  This rule demonstrates
a transformation of both schedule and (if the transposition is
manifested) data of the program being optimized.

Rule~\G{6} allows for distributing a \kw{rearrange} construct by
rearranging the outer array (input to the map nest) with an expanded
permutation.  The semantics of \lstinline{rearrange p a} is that it
returns \lstinline{a} with its dimensions reordered by a
statically-given permutation \kw{p}. For instance, the expression
\mbox{\lstinline{rearrange (2,1,0) a}} reverses the dimensions of the
three-dimensional array \lstinline{a}.
%
For convenience, we use \mbox{\lstinline{transpose a}} as syntactic
sugar for \mbox{\lstinline{rearrange (1,0,...) a}}, which swaps the two
  outermost dimensions.
%
Similar rules can be added to handle other expressions that have a
particularly efficient formulation when distributed on their own,
such as concatenation, but we elide these for brevity.

Finally, rule~\G{7} implements a \lstinline{map}-\lstinline{loop}
interchange.  The
simple intuition is that\\
\hspace*{0.5cm}\lstinline!map (\x -> loop (x'=x) for i < n do (f x')) xs!\\
is equivalent to\\
\hspace*{0.5cm}\lstinline!loop (xs'=xs) for i < n do (map f xs')!\\
because they both produce \texttt{[f$^n$(xs[0]), $\ldots$
  ,f$^n$(xs[m-1])]}.  The rule is sufficiently general to deal with
all variations of variant and invariant variables in the
\lstinline{loop} body.  The side condition in the rule ensures that
$z \subseteq q$ are free variables and thus invariant to $\Sigma$.
The rule is applied only if the body of the loop contains inner
parallelism, such as maps, otherwise its application is not beneficial
(as an example, it would change the Mandelbrot benchmark from
\cref{sec:accelerate} to have a memory- rather than a compute-bound
behavior).
%
However, rule~\G{7} is essential for efficient execution of the
LocVolCalib benchmark (\cref{sec:finpar}), because a loop separates
the outer map from four inner maps.

We conclude by remarking that some of the choices made in the
flattening rewrite rules about how much parallelism to exploit and how
much to sequentialize efficiently are arbitrary, because there is no
size that fits all.  For example, we currently sequentialize a
\StreamPar{} if it is inside a \Map{} nest, but the algorithm can
easily be made more aggressive.  To see the problem, consider an
expression that computes the sum of rows of a matrix:\\
\hspace*{0.5cm}\lstinline{map (\xs -> reduce (+) 0 xs) xss}\\
Assume that \lstinline{xss} is of shape $n\times{}m$.  If $n$ is
sufficiently large (tens of thousands on a GPU), then it is most
efficient to turn the \kw{reduce} into a sequential loop.  If $n$ is
very small, then perhaps it is best to sequentialise the \kw{map}, and
keep the \kw{reduce} parallel.  Or perhaps we must exploit the
parallelism of both the \kw{map} and the \kw{reduce} to fully saturate
the parallel capacity of the target hardware.  Since $n$ and $m$ are
not known at compile-time, we cannot generate code that is optimal for
all cases.

A more general solution would be to generate all three possible code
versions, and to discriminate between them at runtime based on dynamic
predicates that test whether the exploited parallelism is enough to
fully utilize hardware.  This idea is discussed further below.

\section{Multi-Versioned Code}
\label{sec:multi-versioned-code}

The flattening rules of the preceding section are able to directly
decompose \kw{map}-nestings via loop distribution.  However, they are
not in general able to exploit parallelism nested within the
functional argument to a non-\kw{map} SOAC.  Some rules, for example
G5 that performs \kw{reduce}-\kw{map} interchange, expose \kw{map}s
which can then be further handled by the moderate flattening
algorithm.  This section presents further rules that decompose SOACs
into \kw{map}s and other SOACs.  Furthermore, we also introduce a rule
for \textit{multi-versioned code}, by which a single SOAC may be
flattened in two different ways, with either of the two alternatives
picked at runtime via a branch.  While the Futhark compiler contains a
prototype implementation of multi-versioned code, it is not yet used
by default.  Hence, the effectiveness of the ideas in this section are
not shown empirically on benchmarks.



\begin{figure}
\sempart{More Flattening Rules}{\Sigma \vd e \Rightarrow e}

\begin{align*}
  \inference{
  \Sigma \vd e \Rightarrow e'_1\\
  \Sigma \vd e \Rightarrow e'_2 \\
  e'_{1} \neq e'_{2} \\
  v\mbox{ is fresh}
}{
  \Sigma \vd e \Rightarrow \Let{v}{\textit{predicate}}{\If{v}{e'_{1}}{e'_{2}}}
} \tagsc{E1}
\end{align*}

\begin{align*}
\inference{
  \nseq{a}{n}~\nseq{b}{m}~\nseq{c}{n}~\mbox{are fresh} \\
  f' = f~\nseq{v}{n} \\
  op_{1} \equiv \Map~f'~\seq{x}\sp
  op_{2} \equiv \Reduce~\oplus~(\nseq{v}{n})~\nseq{a}{n}\\
  \Sigma \vd \Let{\nseq{a}{n}~\nseq{b}{m}}{op_{1}}{\Let{\nseq{c}{n}}{op_{2}}{(\nseq{c}{n},\nseq{b}{m})}} \Rightarrow e'
}{
\Sigma \vd \Redomap~\oplus~f~(\nseq{v}{n})~\seq{x} \Rightarrow e'
} \tagsc{E2}
\end{align*}

\begin{align*}
  \inference{
  \nseq{a}{n}~\nseq{b}{m}~\nseq{c}{n}~\mbox{are fresh} \\
  op_{1} \equiv \Map~(\textrm{unchunk}(n,m,f))~\seq{x}\sp
  op_{2} \equiv \Reduce~\oplus~(\nseq{v}{n})~\nseq{a}{n}\\
  \Sigma \vd \Let{\nseq{a}{n}~\nseq{b}{m}}{op_{1}}{\Let{\nseq{c}{n}}{op_{2}}{(\nseq{c}{n},\nseq{b}{m})}} \Rightarrow e'
  }{
  \Sigma \vd \StreamPar~\oplus~f~\seq{x} \Rightarrow e'
  } \tagsc{E3}
\end{align*}

\begin{align*}
  \inference{
  w~\textrm{is size of any}~x \\
  \Sigma \vd \Let{c}{w}{\nseq{\kw{let}~v_{i}~=~x_{i}}{k}~\kw{in}~e_{f}} \Rightarrow e'
  }{
  \Sigma \vd \StreamSeq~(\FnU{c~\nseq{v}{k}}{e_{f}})~(\nseq{v}{n})~\nseq{x}{k} \Rightarrow e'
  } \tagsc{E4}
\end{align*}

\caption{Flattening rules that are more aggressive in exploiting inner
  parallelism.  The definition of \textrm{unchunk} can be found on
  \cref{fig:unchunking}.}
\label{fig:more-flattening-rules}
\end{figure}

\begin{figure}

  \begin{align*}
    \textrm{unchunk}(n, m, \Fn{(\nseq{\tau}{n},\nseq{[c]\tau}{m})}{c~\nseq{v}{k}}{e}) =
    & \fn \nseq{v'}{k}: (\nseq{\tau}{n},\nseq{\tau}{m}) \rightarrow \\
    & \quad \kw{let}~c~=~1\\
    & \quad \nseq{\kw{let}~v_{i}~=~[v'_{i}]}{k} \\
    & \quad \kw{let}~\nseq{a}{n}~\nseq{b}{m}~=~e \\
    & \quad \nseq{\kw{let}~b'_{i}~=~b_{i}[0]}{m} \\
    & \quad \kw{in}~\nseq{a}{n}~\nseq{b'}{m}
  \end{align*}
  \caption{Turning a chunk function into a non-chunked function by
    setting the chunk size to unit, and turning the resulting
    unit-size map-out results into single values.}
  \label{fig:unchunking}
\end{figure}

Multi-versioned code does not change the overall framework of moderate
flattening.  Rather, \cref{fig:more-flattening-rules} simply provides
additional rules that are more aggressive in decomposing SOACs into
constituent parts.  The most crucial rule is E1, which states that if
some expression $e$ can be flattened in two distinct ways $e'_{1}$ and
$e'_{2}$, then $e$ can be flattened to an \kw{if} expression that uses
some runtime \textit{predicate} to determine which of $e'_{1}$ or
$e'_{2}$ to apply.  We do not specify the form of the predicate in
detail here.  One typical form is to somehow quantify the amount of
parallelism exploited by $e'_{1}$ versus $e_{2}'$, and picking the
version that exploits the \textit{least} amount of parallelism that
still exceeds some runtime-given constant that characterises the
hardware the program is executing on.  The intuition is that we want
to fully saturate the hardware, but parallelism in excess of that is a
waste.  Determining the right form of the predicates, and the
constants they may employ, is in practice a form of
\textit{auto-tuning}.

Rule E2 describes how a \kw{redomap} can be taken apart into a
\kw{map} and a \kw{reduce}, which is then (potentially) further
flattened.  The \kw{map} uses a modified function $f'$, in which we
fix the first $n$ parameters of the original function $f$
(corresponding to the accumulator) to the neutral value.  Since
\kw{redomap} returns both reduced and mapped results, we are careful
to only pass the former to the \kw{reduce}.  An almost identical rule
can be defined for \kw{scanomap}.

Rule E3 is similar to E2, but applies to \StreamPar{}, and recovers
all available parallelism by splitting into a \kw{map} and a
\kw{reduce}.  The main difference compared to E2 is that computing the
function for \kw{map} is somewhat more complicated, as shown on
\cref{fig:unchunking}.

Rule E4 is used to exploit any parallelism inside of a \StreamSeq{}.
This is done by fixing the chunk size $c$ to the full size $w$ of the
input arrays, and then inserting the function body.

\section{Kernel Extraction}

The moderate flattening algorithm, as presented in this chapter, does
not directly transform SOACs into flat-parallel kernels suitable for
GPU execution.  Instead, it merely rearranges the SOACs into simpler
forms that are passed to a simple pattern matcher, which recognises
forms of \textit{top-level} parallelism, and transforms it to an
explicitly flat kernel representation of GPU kernels.  In this
context, top-level parallelism is a SOAC that is not embedded inside
the function of another SOAC.  We will not describe this operation, or
the full kernel representation used by the Futhark compiler, in
detail, as it pertains to straightforward but tedious book-keeping,
but some details are worth mentioning.

In the implementation, there is no distinction between moderate
flattening and kernel extraction: it is interleaved in the same pass.
Conceptually, when rule G1 first fires, it will sequentialise the
expression $e$, and then manifest the \textit{entire} map context
$\Sigma$ at once, in the form of a flat parallel kernel.

Not all kernels are in the form of a \kw{map} nest containing a
sequential function.  Other patterns that may give rise to a GPU
kernel are:

\begin{itemize}
\item \kw{redomap} or \kw{scanomap} can be transformed into two GPU
  kernels via the techniques used in
  \cref{sec:calculus-implementation}.
\item \StreamPar{} can be transformed into two GPU kernels similarly
  to a \kw{redomap}.  Any parallelism inside the functional arguments
  is sequentialised.
\item \kw{redomap} or \kw{scanomap} perfectly nested within one or
  more \kw{map}s can be transformed into kernels implementing
  segmented reduction or prefix scan.  The number of kernels depends
  on the exact implementation strategy, which is outside the scope of
  this thesis.  The implementation strategy for segmented reductions
  used by the Futhark compiler is presented in
  \cite{Futhark:segredomap}.
\item \kw{scatter} can be transformed directly into a single GPU
  kernel similar to \kw{map}.
\end{itemize}

The bodies of the kernels produced by kernel extraction remain Futhark
code, and kernel extraction maintains the original structure of any
sequential code.  Importantly, even though nested SOACs may not have
had their potential for parallelism exploited, they remain high-level
descriptions of loops, which can be used for further optimisation.  We
shall see examples in \cref{chap:tiling}.

Later in the compiler, the high-level kernel representation is
transformed into a low-level imperative IR, and from there to actual
OpenCL code.  However, these topics are outside the scope of this
thesis.

\section{Related Work}

Futhark builds on previous work in type systems and parallel
compilation techniques. Recent
work~\cite{Steuwer:2015:GPP:2858949.2784754} shows that stochastic
combinations of rewrite rules open the door to autotuning.  Work has
been done on designing purely functional representations for OpenCL
kernels~\cite{Steuwer:2017:LFD:3049832.3049841}, which could in
principle be targeted by our moderate flattening algorithm.  The
Futhark compiler presently uses a similar (but simpler)
representation, the full details of which are outside the scope of
this thesis.

Halide~\cite{Halide} uses a stochastic approach for finding optimal
schedules for fusing stencils by a combination of tiling, sliding
window and work replication.  This is complementary to Futhark, which
does not optimise stencils, nor uses autotuning techniques, but could
benefit from both.  In comparison, Futhark supports arbitrary nested
parallelism and flattening transformation, together with streaming
SOACs that generically encode strength-reduction invariants.

Delite uses rewrite rules to optimize locality in NUMA
settings~\cite{DeliteNUMA} and proposes
techniques~\cite{DeliteNestedPar} for handling simple cases of nested
parallelism on GPUs by mapping inner parallelism to CUDA block and
warp level.  We use transposition to handle coalescing
(see~\cref{chap:tiling}), and, to our knowledge, no other compiler
matches our AST-structural approach to kernel extraction, except for
those that employ full flattening, such as
NESL~\cite{blelloch1994implementation,bergstrom2012nested}, which
often introduces inefficiencies and does not support in-place updates.
%
Data-only flattening~\cite{Bergstrom:2013:DFN:2442516.2442525} shows
how to convert from nested to flat representation of \textit{data},
without affecting program structure.  This would be a required step in
extending Futhark to exploit irregular parallelism.  In comparison,
Futhark flattens some of the top-level parallelism \textit{control},
while preserving the inner structure and opportunities for
locality-of-reference optimizations.

Imperative GPU compilation techniques rely on low-level index
analysis ranging from pattern-matching
heuristics~\cite{InformalTiling,Lime} to general modeling
of affine transformations by polyhedral analysis~\cite{PolyPluto2,PolyhedralOpt}.
%
Since such analyses are often hindered by the expressivity of the
language, solutions rely on user annotations to improve accuracy. For
example, OpenMP annotations can be used to enable transformations of
otherwise unanalyzable patterns~\cite{chatarasi2015polyhedral}, while
PENCIL~\cite{PencilPACT} provides a restricted C99-like low-level
language that allows the (expert) user to express the parallelism of
loops and provide additional information about memory access patterns
and dependencies.

The moderate flattening transformation resembles the tree-of-bands
construction~\cite{PolyPluto2} in that it semantically builds on
interchange and distribution, but we use rules that directly exploit
properties of the language. For example, imperative approaches would
implement a reduction with a vectorized operator via a histogram-like
computation~\cite{RedPencil}, which is efficient only when the
histogram size is small.  In comparison, rule G5 in
\cref{fig:basic-flattening-rules} transforms a reduction with a
vectorized operator to a (regular) segmented reduction, which always
has an efficient implementation.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
