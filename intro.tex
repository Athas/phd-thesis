\chapter{Introduction}

TODO:

\begin{itemize}
\item scope and philosophical level of how our approach is placed compared to imperative approaches

\item show the vision

\item futhark source language examples and introduction

\item show how techniques from imperative compilers become easier in a functional setting

\item talk about all kinds of promising techniques, followed by a compiler that can actually do some of them.
\end{itemize}

This thesis describes the design and implementation of
\textit{Futhark}, a data parallel functional programming language.
Futhark is a small programming language that is superficially similar
to known functional languages such as OCaml and Haskell, but with
restrictions and extensions meant to permit compilation into efficient
parallel code.  While my research has produced techniques that could
be applied in other settings, Futhark has been the overarching context
for my work.  Apart from serving as a vehicle for compiler research,
Futhark is also a programming language that is useful in practice for
high-performance parallel programming.

The remainder of this introductory chapter introduces the challenges
posed by modern high-performance parallel computers.  I will discuss
that while conventional imperative languages once benefited greatly
from their relatively close similarity to the hardware, the divide
behind their conceptual model and the realities of hardware is now
ever widening.  I will show the advantages of a high-level explicitly
parallel programming model, both for expressing parallel algorithms,
as well as for compilation to efficient low-level code.  I will
discuss how techniques considered \textit{heroic effort} in a compiler
for an imperative language become tractable when applied in the
functional setting.

Chapter~\ref{chap:calculus} introduces a formal array calculus that
serves as inspiration for Futhark.  Various rewrite rules are shown
and justified.  Chapter~\ref{chap:hardware} discusses a simplified but
performance-faithful design of contemporary GPU hardware, and shows
how the primitive combinators of the previous chapter can be mapped
efficiently to low-level code.

The second part of the thesis discusses various analyses,
transformations, and optimisations carried out by the Futhark
compiler.  These constitute my main contributions.
Chapter~\ref{chap:size-analysis} shows a technique for array shape
analysis based on slicing and existential types (previously published
at FHPC 2014~\cite{henriksen2014size}).  Chapter~\ref{chap:fusion}
presents the fusion algorithm employed by the fusion compiler ---
parts have been published in FHPC 2013~\cite{henriksen2013t2}, my
master's thesis~\cite{henriksen2014exploiting}, FHPC
2016~\cite{Futhark:redomap}, and PLDI
2017~\cite{henriksen2017futhark}.
Chapter~\ref{chap:kernel-extraction} shows an algorithm that uses
primarily loop distribution to transforming regular nested parallelism
into flat parallelism.  The main novelty is that information (such as
access patterns) is not destroyed to the degree seen in previous
techniques.  Chapter~\ref{chap:tiling} shows an implementation of
automatic \textit{loop tiling} that exploits just this information.
These two chapters are based on work previously published at PLDI
2017~\cite{henriksen2017futhark}.
Chapter~\ref{chap:empirical-validation} contains a performance
analysis of Futhark on more than a dozen established benchmarks.
Finally, Chapter~\ref{chap:interoperability} discusses how to call
Futhark code from programs written in other languages, with Python
used as the case study.

\section{Physical Challenges to Improving CPU Performance}

In physics, the speed of light, denoted $c$, is the ultimate speed
limit of the universe.  Likewise in programming, ``as fast as C'' is
often been used as an indication that some programming language is as
fast as it can possibly be.  In theory it makes no sense to say that a
given programming language is ``slow'' or ``fast'', as these are
merely properties of a particular implementation of the programming
language running on a specific computer.  But in practise, it is clear
that the design of a programming language has an overwhelming
influence on the ease with which a performant implementation can be
constructed.

For decades, the design of languages such as C has permitted the
implementation of compilers that generate efficient code.  One
important reason is that the abstract machine model underlying C, a
sequential random-access register machine, can be mapped easily to
mainstream computers with little overhead.  This means that a
programmer can write code in C and have a reasonable idea of the
performance of the resulting code.  It also means that even a naive
non-optimising C compiler can generate code with good performance,
although it may require somewhat more care from the C programmer.
Indeed, C has often been described as ``portable assembly
code''.\footnote{I will using C as the main point of reference in the
  following sections, but the problems I point out are common to any
  sequential language.}

In contrast, languages whose abstract model is more different from
physical machines, so-called ``high-level languages'', cannot be as
easily mapped to real machines.  Compilers must invest considerable
effort in mapping e.g. the call-by-need lambda calculus of Haskell to
the sequential register machine~\cite{jones1992implementing}.  Even
after decades of work, high-level languages struggle to catch up to
the performance of C on the sequential random-accesss register
machine.

Of course, a cursory study of modern hardware designs show that the
sequential random-access register machine is a lie.  The abstract
model of C may match a 70s minicomputer reasonably well, but it is
increasingly diverging from how modern computers are constructed.
This is not despite lack of effort on behalf of the hardware
designers: due to the massive popularity (and therefore economic
significance) of C-like languages, manufacturers have attempted to
prolong the lie of the sequential random-access machine for as long as
possible.

Unfortunately, while C may be exceeded (as I hope to show in this
thesis), $c$ is a harder nut to crack.  The machine on which I am
typing contains a CPU with a clock frequency of $2.3GHz$,
corresponding to a clock cycle taking $0.43ns$.  Given that $c$ is
approximately $3\cdot10^{8}m/s$, light moves approximately $13cm$ in
the the time the CPU takes to perform a clock cycle, thus physically
limiting the distance a single piece of information can travel in a
single cycle.

Another physical limitation is power dissipation: roughly, the power
usage of the transistors in a processor is roughly proportional to the
square of its clock frequency.  This prevents an increase in clock
frequency unless we can compensate with an increase in the efficiency
of the transistors.  For a sequential machine that executes
instructions in exactly the order they are given, the only way to
speed up execution of a program is to increase the rate at which
instructions are executed.  If we can double the rate at which
instructions are processed, we in effect halve the time it takes to
execute some program (assuming no memory bottlenecks, which I will
discuss later).

In the popular vernacular, \textit{Moore's Law} is typically rendered
as ``computers double in speed every two years''.  But actually, the
law states that \textit{transistor density} doubles every two years
(implying that transistors become smaller).  Moore's Law does not
state that the enlargened transistor budget translates
straightforwardly to improved performance, although that was indeed
the case for several decades.  This is due to another law,
\textit{Dennard scaling}, which roughly states that as transistors get
smaller, their power density stays constant.  Taken together, Moore's
Law and Dennard scaling rougly say that we can put ever more
transistors into the same physical size, and with the same power usage
(and thus heat generation).

The reduction in power usage per transistor granted by Dennard scaling
permitted a straightforward increase in CPU clock frequency.  Roughly,
every time we cut the power consumption of a transistor in half via
shrinkage, we can increase the clock frequency by 50\%.  For a
sequential computer, this translates into a 50\% performance increase.
We can partially circumvent the limits of $c$ by using techniques such
as \textit{pipelining}.  While it is clear that Moore's Law will
eventually stop, or else transistors would eventually be smaller than
a single atom, this is not the issue that has hindered the lie of the
sequential machine.

The problem is that Dennard scaling began to break down around 2006.
Physical properties of the circuit material leads to increased current
leakage at small sizes, causing the chip to heat up.  Simply pushing
up the clock frequency is no longer viable.  Instead, chip
manufacturers use the transistor budget on increasing the amount of
work that can be done in a clock cycle through various means:

\begin{description}
\item[Increasing the size of caches.] One significant problem with
  modern computer designs is that processors are significantly faster
  than the memory from which they get their data.  As a rule of thumb,
  accessing main memory incurs a latency of $100ns$ - likely hundreds
  of clock cycles on a current CPU.  This so-called \textit{memory
    wall} can be circumvented by the addition of caches that store
  small subsets of the larger memory.  Heuristics, usually based on
  temporal locality, are useed to determine which parts of the larger
  memory are stored in the caches.  High-performance CPUs can use
  several layers caches, each layer being larger and slower than the
  former.
\item[Inferring instruction-level parallelism.]  While the semantics
  of a sequential CPU is that instructions are executed one after
  another, it is often the case that two instructions to not have
  dependencies on one another, and thus can be executed in parallel.
  CPUs even perform \textit{out-of-order} execution where later
  instructions are executed before earlier ones, if the latter
  instruction has no dependence on the latter.
\item[Explicit parallelism at the hardware level.]  The two former
  techniques try to masquerade the fact that the sequential machine
  model is increasingly physically untenable.  Another approach is to
  explicitly provide programmers with hardware support for parallel
  execution.  This most famously takes the form of adding additional
  CPU cores, each of which processes a sequential stream of
  instructions, but an equally important technique is to add
  \textit{vector instructions} that operate on entire vectors of data,
  rather than single values.  As an example, the newest Intel AVX-512
  vector instruction set provides instructions that operate on entire
  512-bit vectors (for example, 16 single precision floating point
  values).  Such instructions dramatically increase the amount of work
  that is done in a single clock cycle.
\end{description}

These techniques are all based on merely extending and improving the
classic CPU design.  While code may have to be re-written to take
advantage of certain new hardware features (such as multiple cores or
vector instructions), the programming experience is not too dissimilar
from programming sequential machines.  This is not necessarily a bad
thing: sequential machines are easy to reason about, generous in their
support of complex control flow, and have a significant existing base
of programmer experience.

However, the notion that C is ``portable assembly'' begins to crack
noticeably even at this point.  C itself has no built-in notion of
multi-threaded programming, nor of vector operations, and thus vendors
have to provide new programming APIs (or language extensions) for
programmers to take advantage of the new (performance-critical)
machine features.  Alternatively, C compilers must perform significant
static analysis of the sequential C code to find opportunities for
vectorisation or multithreading.  In essence, the compiler must
reverse engineer the sequential statements written by the programmer
to reconstruct whatever parallelism may be present in the original
algorithm.

The problem is that C was carefully designed for a particularly class
of machines; a machine no longer resembled by modern high-performance
computers.  Thus, the \textit{impedance mismatch} between C and
hardware continues to grow, with little sign of stopping.  This
mismatch becomes acute when we move away from the comparatively benign
world of multicore CPUs and into the realm of massively parallel
processors.

\section{Massively Parallel Machines}

To a large extent, mainstream CPUs favour programming convenience,
familiarity, and backwards compatibility over raw performance.  We
must look elsewhere for examples of the kind of hardware that could be
built if we were willing to sacrifice traditional sequential notions
of sequential programming.

For as long as there has been computers, there have been parallel
computers.  Indeed, if we think back to the very earliest computers,
the human ones made of flesh and blood, parallelism was the
\textit{only} way to speed up a computation.  You were unlikely to
make a single human computer much faster through training, but you
could always pack more of them into a room.  However, with the rise of
electrical computers, and in particular the speed with which
sequential performance improved, parallelism dwindled into the
background.  Certainly, you could have two computers cooperate on
solving some problem, but the programming difficulties involved made
it more palatable to simply wait for a faster computer to enter the
market.  Only the largest computational problems were worth the pain
of parallel programming.\footnote{\textit{Concurrency} was alive and
  well, however, due to its significant importance in operating
  systems and multiprocessing.  But most concurrent programs were
  executed on sequential machines, with the illusion of concurrency
  formed through multiplexing.  Many of the techniques developed for
  concurrent programming are also applicable to parallel programming,
  although concurrency tends to implicitly focus on correctness over
  performance on some specific machine.}

Several interesting parallel computers were designed for
\textit{high-performance computing} (HPC) applications.  One of the
earliest and most influential was the Cray-1 from 1976, which was the
first vector processor.  Only about 100 were sold, but this is a large
number for a high-end supercomputer.  A different design was the CM-1
from 1985, which was based on a computer containing a large number of
simple 1-bit microprocessors.  The CM-1 proved difficult to program
and was not a success in the market, but was the first example of a
\textit{massively parallel machine}.

In the 90s, consumer demand for increasing visual fidelity in video
games led to the rise of the \textit{graphics processing unit} (GPU),
for accelerating graphical operations that would be too slow if
executed on the CPU.  Initially, GPUs were special-purpose
non-programmable processors that could only perform fixed graphical
operations.  Over time, the need for more flexibility in graphical
effects lead to the development of programmable \textit{pixel
  shaders}, first seen in the NVIDIA GeForce 3 in 2000.  Roughly,
pixel shaders allowed an effect to be applied to every pixel of an
image - for example, looking at every pixel and add a reflection
effect to those that represent water.  Graphical operations such as
these tend to be inherently parallel, and as a result, GPU hardware
evolved to support efficient execution of programs with very simple
control flow, but a massive amount of fine-grained parallelism.
Eventually, GPU manufacturers started providing APIs that allowed
programmers to exploit the parallel computational power of the
now-mainstream GPUs even for non-graphics workloads, so-called
\textit{general-purpose GPU programming} (GPGPU).  The most popular
such APIs are CUDA~\cite{cuda} from NVIDIA, and
OpenCL~\cite{Stone:2010:OPP:622179.1803953}.

GPUs are not the only massively parallel machines in use.  However,
their great success, their availability, the difficulty in programming
them, and their potential compute power makes them excellent objects
of study for researchers of parallel programming models.  In this
work, I focus on the details of GPUs over other parallel machines.
However, a central thesis of the work is that modern hardware is too
complicated and too diverse for low-level programming by hand to be
viable.  I will introduce a high-level hardware-agnostic programming
model, and describe its efficient mapping to GPUs.  The implication is
that if the model can be mapped efficiently to a platform as
restricted as GPUs, it can probably also be mapped to other parallel
platforms, such as multicore CPUs or FPGAs.

\subsection{Basic Properties of GPUs}

The performance characterictics and programming model of modern GPUs
is covered in greater detail on~\ref{chap:hardware}, but a basic
introduction is given here, in order to give an idea of the
difficulties involved in retrofitting sequential languages for GPU
execution.

GPUs derive their performance from an execution model called
\textit{single instruction multiple thread} (SIMT), which is very
similar to the \textit{single instruction multiple data} model.
Roughly, threads are not fully independent, but grouped into bundles
that all execute the same operations on different parts of a large
data set.  For example, on an NVIDIA GPU, threads are bundled into
\textit{warps}, of 32 threads each, that execute in lockstep, and
which form the unit of scheduling.  This execution is highly suited
for dense and regular computations, such as the ones found in linear
algebra, but less so for irregular and branch-heavy computations such
as graph algorithms.

A GPU is still subject to the laws of physics, and there is therefore
a significant latency between issuing a memory read, and actually
receiving the requested data (the memory wall).  On a CPU, a hierarchy
of caches is used to decrease the latency, but a GPU uses aggressive
\textit{simultaneous multithreading}, where a thread that is waiting
for a memory operation to complete is de-scheduled and another thread
run in its place.  This scheduling is done entirely in hardware, and
thus does not carry the usual overhead of context switches.  This
scheduling is not done on each thread in isolation, but on entire
32-thread warps.  Thus, while a GPU may be only have enough
computational units (such as ALUs) to execute a few thousand parallel
instructions per clock cycle, tens of thousands of threads may be
necessary to avoid the computational units being idle while waiting
for memory operations to finish.  As a result of this design, memory
can also be optimised for very high bandwidth at the expense of
latency, and it is not unusual for GPU memory buses to support
bandwidth in excess of $300GiB/s$ (although as we shall discuss in
Chapter~\ref{chap:hardware}, specific access patterns must be followed
to reach this performance).

GPUs have many limitations that hinder traditional programming
techniques and languages:

\begin{itemize}
\item GPUs function as \textit{co-processors}, where code is
  explicitly uploaded and invoked.  A GPU program is typically called
  a \textit{GPU kernel}, or just \textit{kernel}.
\item The amount of threads necessary implies that each thread can use
  only relatively few registers and little memory (including stack
  space).
\item The lockstep execution of warps, as well as the very small
  per-thread stack size, prevents function pointers and recursion.
\item GPUs cannot directly access CPU memory.\footnote{This is changing with the newer generation of GPUs.}
\item Memory allocation is typically not possible while executing a
  GPU program.  All necessary memory must be pre-computed before
  starting GPU execution.
\item Generally no support for interrupts or signals.
\end{itemize}

The mismatch between the abstract machine model of C and GPU
architectures is clear.  While current GPU programming APIs do in fact
use a restricted subset of C (or C++ in the case of CUDA) for
expressing the per-thread code, it is the programmmer's responsibility
to orchestrate their execution.  This involves organising
communication between the tens of thousands of threads that are needed
to saturate the hardware---a tall order, and the compiler cannot help.

\section{A Parallel Programming Language}

We need a better programming model; one that does not have as
fundamental a mismatch between its execution model and parallel
hardware.  This does not mean we need a low-level language, or a
specialised GPU language.  Rather, we should \textit{increase} the
level of abstraction, and stop overspecifying details like iteration
order and sequencing, unless we really \textit{need} something to
execute in some specific order.  In a C program, everything is
required to execute in exactly the order written by the programmer,
and the C compiler must perform significant work to figure out which
of the sequencing restrictions are essential, and which are
accidental.  In the following, I will introduce the \textit{Futhark
  programming language}, a purely functional array language that has
been developed as part of my PhD work\footnote{Futhark has its roots
  in an earlier language, $\mathcal{L}_0$, which I helped develop
  during my master's studies.}.

A reasonable high-level programming model for modern computers is one
based on \textit{bulk operations}, where we program in terms of
large-scale transformations of collections of data.  Suppose we wish
to increment every element in a vector by $2$.  The functional
programming tradition comes with an established vocabulary of such
bulk operations which we can use as inspiration.  For this task, we
can use the \lstinline{map} construct, which takes a function
$\alpha~\rightarrow~\beta$ and an array of values of type $\alpha$,
and produces a collection of values of type $\beta$:

\begin{lstlisting}
map (\x -> x + 2) xs
\end{lstlisting}

The above is a valid expression in the Futhark programming language.
We use the notation \lstinline{(\x -> ...)}, taken from Haskell, to
express an anonymous function with a parameter \lstinline{x}.  This
expression does not specify \textit{how} the computation is to be
carried out, and it does not imply any accidental ordering
constraints.  Indeed, the only restriction is that this expression can
only be evaluated after the expression that produces \lstinline{xs}.

In this section, and those that follow, Futhark will be used to
demonstrate the qualities of parallel programming models.  Futhark is
by no means the first parallel programming language, nor even the
first parallel functional language.  That title likely belongs to the
venerable APL, which was first described in 1962~\cite{iversonbook}.
Futhark is not even the first parallel language in a
$\lambda$-calculus style, as it is predated by
NESL~\cite{BlellochCACM96NESL} by some twenty years.  Indeed, as we
shall see, Futhark is not even the most expressive such language.
Futhark has more similarities than differences from other parallel
functional languages.  The main contributions of this thesis are the
\textit{implementation techniques} that have been developed for
Futhark---its efficient mapping to GPU hardware---as well as those
bits of its language design that enable said implementation.  It seems
likely that the implementation techniques could be applied to other
functional languages of roughly the same design.  For more on why I
chose to construct a new programming language rather than use an
existing design, see Section~\ref{sec:new-language}.

Let us return to the \lstinline{map} expression:

\begin{lstlisting}
map (\x -> x + 2) xs
\end{lstlisting}

The style of parallelism used by Futhark is termed \textit{explicit
  data parallelism}.  It is \textit{explicit} because the user is
required to use special constructs (here, \lstinline{map}) to inform
the compiler of where the parallelism is located, and it is
\textit{data parallel} because the same operation is applied to
different pieces of data.  In contrast, thread-based programming is
based on \textit{task parallelism}, where different threads perform
different operations on different pieces of data (or a shared piece of
data, if one enjoys debugging race conditions).  Task parallelism does
necessarily imply low-level and racy code with manual synchronisation
and message passing, but can be given safe structure through
e.g. futures or fork/join patterns.

One important property of functional data parallelism is that the
semantics are sequential.  The program can be understood entirely as a
serial sequence of expressions evaluating to values, with parallel
execution (if any) not affecting the result in any way.  This
decoupling of semantics and operations is typical of functional
languages, and is key to enabling aggressive automatic transformation
by a compiler.  It is also easier for programmers to reason
sequentially than in parallel.  It is still important to the
programmer that an operation such as \lstinline{map} is, however,
operationally parallel can be described through parallel cost models.
One such cost model was described and subsequently proven
implementable for the data parallel language
NESL~\cite{Blelloch:1996:PTS:232627.232650}.  While a formal cost
model for Futhark is outside the scope of this thesis, the similarity
of Futhark to NESL suggests that a similar approach would be viable.
Instead, we use the intuitive notion that certain constructs (such as
\lstinline{map}) may be executed in parallel.  However, it is not
always efficient to translate all \textit{potential} parallelism into
\textit{realised parallelism} on a concrete machine (see
Section~\ref{sec:efficient-sequentialisation}).

\subsection{Choice of Parallel Combinators}

A persistent question when designing a programming language is which
constructs to build in, and which to derive from more primitive forms.
In functional languages, we usually prefer to include just a few
powerful foundational constructs, on which the rest of the language
can be built.  In principle, \lstinline{map} can be used to express
(almost) all other data-parallel constructs.  For example, we can
write a summation as follows:

\begin{lstlisting}
let sum (xs: []i32): i32 =
  let ys' =
    loop ys=xs while length ys > 1 do
      let n = length ys / 2
      in map (+) (zip ys[0:n] ys[n:2*n])
  in ys'[0]
\end{lstlisting}

An explanation of the syntax is necessary.  In Futhark, both functions
and local bindings are defined using the \lstinline{let} keyword.
These functions both take a single parameter, \lstinline{xs} of type
\lstinline{[]i32}.  We write the type of arrays containing elements of
type \lstinline{t} as \lstinline{[]t}.  The \lstinline{loop} construct
is specialised syntax for expressing sequential loops.  Here,
\lstinline{ys} is the \textit{variant parameter}, which initialised
with the value \lstinline{xs}, and receives a new value after every
iteration of the loop body.  The array slicing syntax
\lstinline{ys[n:2*n]} takes elements starting at index \lstinline{n}
and up to (but exclusive) \lstinline{2*n}.

The summation proceeds by repeatedly cutting the array in twain,
adding each half to the other until only a single element is left.
For simplicity, we assume that the size of the input array
\lstinline{xs} is a power of two.  We can characterise the performance
of \lstinline{sum} with with a work/depth parallel cost
model~\cite{Blelloch:1995:PSF:224164.224210}.  If we suppose that
\lstinline{map} runs in work $O(n)$ and depth $O(1)$, the function
\lstinline{sum} runs in work $O(n)$ and depth $O(\log n)$.  This is
asymptotically optimal for physically reasonable machine models.
However, if executed straightforwardly on a GPU, this function will be
far from reaching peak potential performance.  One reason is that it
is very expensive to create the intermediate \lstinline{ys} arrays,
compared to the time taken to add the numbers.  While a ``sufficiently
smart'' compiler may be able to rewrite the program to avoid some of
this overhead (and perhaps even take advantage of hardware-specific
features for performing summations), this is contrary to the
philosophy behind Futhark.  Although we do not mind aggressive
optimisation, the transformations we perform should arise naturally
out of the constructs used by the programmer, and not depend on subtle
and fragile analyses.

As a consequence, we provide several parallel constructs in Futhark
that, while expressible in an asymptotically optimal form using
\lstinline{map} and sequential loops, can be mapped by the compiler to
far superior low-level code.  We still wish to keep the number of
constructs small, because each requires significant effort to
implement and integrate in the compiler, particularly with respect to
optimisations such as fusion (Chapter~\ref{chap:fusion}).  The main
constructs we have chosen to include are the following, which closely
resemble higher-order functions found in most functional languages:

\begin{itemize}
\item \lstinline{map} : $(\alpha\rightarrow\beta)\rightarrow\texttt{[]}\alpha\rightarrow\texttt{[]}\beta$\hfill\\
  \lstinline{map f xs} applies the function \lstinline{f} to every
  element of the array \lstinline{xs}, producing a new array.
\item \lstinline{reduce} : $(\alpha\rightarrow\alpha)\rightarrow\alpha\rightarrow\texttt{[]}\alpha\rightarrow\alpha$\hfill\\
  \lstinline{reduce f v xs} Reduce the array \lstinline{xs} with the function \lstinline{f}.
  The function must take two arguments and be associative, with
  \lstinline{v} as the neutral element.
\item \lstinline{scan} : $(\alpha\rightarrow\alpha)\rightarrow\alpha\rightarrow\texttt{[]}\alpha\rightarrow\texttt{[]}\alpha$\hfill\\
  \lstinline{scan f v xs} computes an \textit{inclusive scan}
  (sometimes called \textit{generalised prefix sum}) of the array
  \lstinline{xs}.  As with reduction, the function must be associative
  and have \lstinline{v} as its neutral element.
\item \lstinline{filter} : $(\alpha\rightarrow\texttt{bool})\rightarrow\texttt{[]}\alpha\rightarrow\texttt{[]}\alpha$\hfill\\
  \lstinline{filter f xs} produces an array consisting of those
  elements in \lstinline{xs} for which the function \lstinline{f}
  returns \lstinline{true}.
\end{itemize}

Together, constructs such as these are called \textit{second-order
  array combinators} (or \textit{SOAC}s).  Futhark does not presently
permit the programmer to write their own higher-order functions, so
new ones cannot be defined (this can be worked around via higher-order
modules; see Section~\ref{sec:new-language}).  Futhark also contains
two more exotic SOACs, \lstinline{stream_red} and
\lstinline{stream_map}, which will be discussed later.\fixme{when?}

Requiring associativity and neutral elements for the functional
arguments in \lstinline{reduce} and \lstinline{scan} is what enables a
parallel implementation.  For reductions, performance can be improved
further if the operator is also commutative\fixme{add reference}.  For
simple functions, the Futhark compiler can detect commutativity, but
otherwise the programmer can use a special \lstinline{reduce_comm}
SOAC that behaves exactly like \lstinline{reduce}, but carries the
promise that the function is commutative.  There is no such variant
for \lstinline{scan}, because scans do not benefit from commutative
operators.

In all cases, it is the programmers responsibility to ensure that the
functions have the required properties - the compiler will not check.
Indeed, checking such properties in general is undecidable.  If an
invalid function is provided (e.g. subtraction, which is not
associative), the program may produce nondeterministic results.

In some parallel languages or libraries, reductions and scans are
restricted to a small collection of standard operators, like addition
or maximum.  This makes the language safer, but we have found many
examples of problems that can be solved efficiently with nonstandard
reductions.  For example, consider the problem of finding the index of
the largest element in an array of floating-point numbers.  In many
parallel libraries, this is a primitive operation.  In Futhark, we can
express this as:

\begin{lstlisting}
let index_of_max [n] (xs: [n]f32): i32 =
  let (_, i) =
    reduce_comm
      (\(x,xi) (y,yi) ->
        if      xi <  0 then (y, yi)
        else if yi <  0 then (x, xi)
        else if  x <  y then (y, yi)
        else if  y <  x then (x, xi)
        else if xi < yi then (y, yi)
        else                 (x, xi))
      (0.0, -1)
      (zip xs [0...n-1])
  in i
\end{lstlisting}

First, another note on syntax.  The \textit{size parameter}
\lstinline{[n]} indicates that the function is polymorphic in some
size \lstinline{n}.  This parameter is in scope as a variable of type
\lstinline{i32} in the remaining parameters and body of the function.
We use this to indicate that the array parameter \lstinline{xs} has
\lstinline{n} elements, and to construct an array of the integers from
0 to \lstinline{n-1} in the expression \lstinline{[0...n-1]}.  We will
return to size parameters in Section~\ref{sec:new-language}.

The \lstinline{index_of_max} function functions by pairing each
element in \lstinline{xs} with its index, then performing a reduction
over the resulting array of pairs.  As a result, the two taken
parameters by the reduction function themselves consist of two values.
We consider the neutral element to be any pair where the index is
negative.  To make the operator commutative, we use the indices as a
tie-breaker in cases where multiple elements of \lstinline{xs} have
the same value (the element with the greater index is chosen).  We use
\lstinline{reduce_comm} instead of plain \lstinline{reduce} to inform
the compiler that the operator is indeed commutative.

\subsection{Efficient Sequentialisation}
\label{sec:efficient-sequentialisation}

In the literature, a \textit{parallelising compiler} is a compiler
that takes as input a program written in some sequential language,
typically C or Fortran, and attempts to automatically deduce
(sometimes with the help of programmer-given annotations) which loops
are parallel, and how best to exploit the available parallelism.  A
large variety of techniques exist, ranging from sophisticated static
approaches based on loop index analysis~\cite{PolyhedralOpt}, to
speculative execution that assumes all loops are parallel, and
dynamically falls back to sequential execution of the assumption fails
at runtime~\cite{SpLSC}.  The Futhark compiler is \textit{not} such a
parallelising compiler.  Instead, we assume that the programmer has
already made all parallelism explicit via constructs such as
\lstinline{map} (and others we will cover).  In this style of
programming, it is likely that the program contains \textit{excess
  parallelism}, that is, more parallelism than the machine needs.  As
almost all parallelism comes with a cost in terms of overhead, one of
the main challenges of the Futhark compilers is to figure how much of
this parallelism to actually take advantage of, and how much to turn
into low-overhead sequential code via \textit{efficient
  sequentialisation}.  It is therefore more correct to say that the
Futhark compiler is a \textit{sequentialising compiler}.

For example, let us consider the \lstinline{reduce} construct, which
is used for transforming an array of elements of type $\alpha$ into a
single element of type $\alpha$:

\begin{lstlisting}
reduce (\x y -> x + y) 0 xs
\end{lstlisting}

We require that the functional argument is an associative function,
and that the second element is a neutral element for that function.
The conventional way to illustrate a parallel reduction is via a tree,
as on Figure~\ref{fig:tree-summation}.  To reduce an $n$-element array
$[x_{1},\ldots,x_{n}]$ using operator $\oplus$, we launch $n/2$
threads, with thread $i$ computing $x_{2i}\oplus{}x_{2i+1}$.  The
initial $n$ elements are thus reduced to $n/2$ elements.  The process
is repeated until just a single value is left--the final result of the
reduction.  We perform $O(\log(n))$ partial reductions, each of which
is perfectly parallel, resulting in a work depth of
$O(\log(n))$.

Tree reduction is optimal on an idealised perfectly parallel machine,
but on real hardware, such as GPUs, it is inefficient.  The
inefficiency is caused by exposing more parallelism than needed to
fully exploit the hardware.  The excess parallelism means we pay an
unnecessary overhead due to communication cost between threads.  For a
summation, the overhead of communicating intermediate results between
processors significantly dominates the cost of a single addition.
Efficient parallel execution relies on exposing as much parallelism as
is needed to saturate the machine, but no more.

\begin{figure*}
  \centering
  Here goes a graph.
  \caption{Summation as a tree reduction.}
  \label{fig:tree-summation}
\end{figure*}

\begin{figure*}
  \centering
  Here goes a graph.
  \caption{Summation as a chunked tree reduction.}
  \label{fig:chunked-summation}
\end{figure*}

The optimal amount of parallelism depends on the hardware and exact
form of the reduction, but suppose that parallel $k$ threads are
sufficient.  Then, instead of spawning a number of threads dependent
on the input size $n$, we always spawn $k$ threads.  Each thread
sequentially reduces a chunk of the input consisting of $\frac{n}{k}$
elements, producing one intermediate result per thread.  We then
launch a second reduction over all these intermediate results.  This
second reduction can also be done in parallel, or can be sequential if
$k$ is sufficiently small.  On a sequential machine, we can simply set
$k=1$, and not exploit any parallelism at all.  Efficient
sequentialisation is particularly important (and also more difficult)
when it comes to handling nested parallelism, as we shall see in
Section~\ref{sec:nested-parallelism}.

Efficient sequentialisation is not a single implementation technique,
but a general implementation philosophy, which gives rise to various
techniques and design choices.  It is a principle that I shall often
return to during this thesis, as it has proven critical for executing
Futhark efficiently on real hardware.  In essence, efficient
sequentialisation is used to bridge the impedance mismatch between the
``perfectly parallel'' abstract machine assumed by Futhark, and real
machines that all have only limited parallelism.  As we shall see,
using efficient sequentialisation to move from perfect parallelism to
limited parallelism is much easier, than the the efforts parallelising
compilers go to when converting sequential code to parallel code.

\section{Fusion for Modular Programming}

One of the most important goals for most programming languages is to
support modular and abstract programming.  A modular program is
composed of nominally independent subcomponents, which are composed to
form a full program.  The simplest feature that supports modular
programming is perhaps the \textit{procedure}.  Almost all programming
languages support the subdivision of a program into procedures,
although most languages also support higher-level abstractions.  The
most important property of a procedure is that it can be understood in
terms of its specification, rather than its implementation.  This
simplifies reasoning by abstracting away irrelevant details.

As a purely functional language, procedures in Futhark are called
\textit{functions}.  To support a programming style based on the
writing of small, reusable components, it is important that there is
little run-time overhead to the use of functions.  Function inlining
is a well established technique to remove the overhead of function
calls, although at the cost of an increase in code size.  Another
useful property of inlining is that it enables further optimisation.,
When an opaque function call is replaced with the function body,
further simplification may be possible.  While wide-spread techniques
such as copy propagation, constant folding, and dead code removal
remain useful in a data-parallel such as Futhark, other, more
sophisticated, transformations are also important.  One such
transformation is \textit{loop fusion}, which removes intermediate
results by combining several loops into one.  The mechanisms behind
fusion the Futhark compiler are discussed in detail in
Chapter~\ref{chap:fusion}. The remainder of this section discusses the
intuition and motivation behind loop fusion, as well as showing how
fusion is significantly easier in the functional setting than for
imperative languages.

Let us consider two Futhark functions on arrays:

\begin{lstlisting}
let arr_abs (xs: []i32) = map i32.abs xs

let arr_incr (xs: []i32) = map (+1) xs
\end{lstlisting}

The function \lstinline{arr_abs} applies the function
\lstinline{i32.abs} (absolute value of a 32-bit integer) to every
element of the input array.  The function \lstinline{arr_scale}
increases every element of the input by \lstinline{1}.  We use a
shorthand for the functional argument: \lstinline{(+1)} is equivalent
to \lstinline{\x -> x + 1}, similarly to the \textit{operator
  sections} of Haskell.

Consider now the following expression:

\begin{lstlisting}
  let ys = arr_abs xs
  in arr_incr ys
\end{lstlisting}

If we inline \lstinline{arr_abs} and \lstinline{arr_incr} we obtain:

\begin{lstlisting}
  let ys = map i32.abs xs
  in map (+1) ys
\end{lstlisting}

If we suppose a straightforward execution, the first \lstinline{map}
will read each element of \lstinline{xs} from memory, compute its
absolute value, then write the results back to memory as the array
\lstinline{ys}.  The second \lstinline{map} will then read back the
elements \lstinline{ys} array, perform the \lstinline{(+1)} operation,
and place the result somewhere else in memory.  If \lstinline{xs} has
$n$ elements, the result is a total of $4n$ memory operations.  Given
that memory access is often the bottleneck in current computer
systems, this is wasteful.  Instead, we should read each element of
the array \lstinline{xs}, apply the combined function \lstinline{(\x -> i32.abs x + 1)}, then write the final result, for a total of $2n$
memory operations.  We could write such a \lstinline{map} manually,
but we would lose modularity, as the program is no longer structured
as a composition of re-usable functions.  For such simple functions as
are used in this example, the loss is not great, but the issue remains
for more complicated functions.

The compiler employs producer-consumer \textit{loop fusion} to combine
the two \lstinline{map} operations into one.  The validity of fusion
is in this case justified by the algebraic rule
\[
  \text{map}~f~\circ~\text{map}~g=\text{map}~(f~\circ~g)
\]
This permits the Futhark compiler to automatically combine the two
\lstinline{map}s and produce the following program:

\begin{lstlisting}
  map (\x -> let y = i32.abs x in y + 1) xs
\end{lstlisting}

Fusion is \textit{the} core implementation technique that permits code
to be written as a composition of simple parallel operations, without
having to actually manifest most of the intermediate results.  It is
worth noting that the fusion algorithm used by the Futhark compiler
will never change the asymptotic behaviour of programs.  Thus, a
program that is fully fused will only be a constant amount faster than
one that is not fused at all.  This is in fact a \textit{feature}, as
it means the tractability of a program does not depend on a compiler
optimisation.  Chapter~\ref{chap:fusion} goes into more detail.

While the Futhark programming language does not correspond directly to
any specific calculus, it is heavily based on the array combinator
calculus discussed in Chapter~\ref{chap:calculus}.  This calculus
serves as inspiration and justification for rewrite rules that are
exploited to transform user-written programs into forms that are more
efficient.

\fixme{More stuff - map-reduce and such.}

While loop fusion is not an unknown technique in compilers for
imperative languages, it is significantly more complicated to
implement.  One major problem is that imperative languages do not have
\lstinline{map} as a fundamental construct.  Hence, index analysis is
first needed to determine that some loop in fact encodes a
\lstinline{map} operation, as the following imperative pseudocode
demonstrates:

\begin{lstlisting}
for i < n:
  ys[i] <- f(xs[i])
for i < n:
  zs[i] <- g(ys[i])
\end{lstlisting}

While index analysis can easily become undecidable, it is feasible for
simple cases, such as this one.  A bigger problem is that there is no
guarantee that the loops can be executed parallel.  For example, the
functions \lstinline{f} and \lstinline{g} may have arbitrary side
effects, which means that they must be executed in order.  Many
functions written in an imperative language, even those that are
externally pure, use side-effects internally, for example for storage
management or accumulator variables in loops.  It can be difficult for
a compiler to automatically determine that a function is indeed pure.
A solution is to have the programmer manually add a purity annotation
to the function, which is then trusted by the compiler.  There are two
problems with this technique: first, the programmer may be wrong,
which may result in unpredictably wrong code, depending on how the
optimiser exploits the information.  Second, optimising compilers are
notoriously bad at providing feedback about when insufficient
information inhibits an information, and how the programmer can
rectify the problem.  A performance-conscious programmer may end up
liberally sprinkling purity annotations on most of their functions in
the hope of helping the optimiser, thus exacerbating the first
problem.

Even if we can somehow determine \lstinline{f} and \lstinline{g} to be
pure, the in-place assignment to \lstinline{y} may have an effect if
\lstinline{xs} and \lstinline{ys} are aliases of each other
(i.e. overlap in memory).  Alias analysis is one of the great
challenges for compiler optimisation in imperative
languages~\cite{hendren1992designing}---indeed, the guarantee that two
arrays cannot alias each other are one of the performance benefits
Fortran has over languages such as C.  Modern languages tend to
support annotations by which the programmer can indicate that some
array has no aliases in scope\footnote{The \texttt{restrict} keyword in C99.}.
These annotations, while useful, have the same issues as the purity
annotations discussed above.

In a purely functional language, we avoid these issues by
construction.  This allows the compiler (and the compiler writer!) to
focus on exploiting parallel properties, rather than proving them.

\section{Nested Parallelism}
\label{sec:nested-parallelism}

In a parallel expression \lstinline{map f xs}, the function
\lstinline{f} can in principle be anything.  In particular,
\lstinline{f} can contain more parallelism.  When one parallel
construct can be nested inside of another, we call it \textit{nested
  parallelism}.

The need for nested parallelism arises naturally out of our desire to
support modular programming.  We should be able to map any function
\lstinline{f}, even if \lstinline{f} is parallel itself.  Furthermore,
the parallelism inside of \lstinline{f} should also be utilised---it's
not enough to exploit only the outermost level of parallelism, as that
may not be enough to saturate the hardware.  Unfortunately, it turns
out that nested parallelism is difficult to implement efficiently in
its full generality.  The reason is that it is hard to map arbitrary
nested parallelism to current parallel hardware, which supports only a
fixed level of parallelism efficiently (and typically with harsh
restrictions on the size of each level beyond the first; see
Chapter~\ref{chap:hardware}).  This is an example of an impedance
mismatch between free-form nested parallel program and the real
hardware we have available to us.  Fortunately, there are ways to
bridge the divide that follow straightforwardly from the construction
of data-parallel functional programs in general, and Futhark programs
in particular.

Guy Blelloch's seminal work on NESL demonstrated how to handle
arbitrary nested parallelism via \textit{full
  flattening}~\cite{blelloch1994implementation}.  The flattening
algorithm transforms arbitrary nested data parallelism into flat data
parallelism, which can be easily mapped on to most machines.  The key
technique is \textit{vectorisation}, by which each function $f$ is
lifted to a vectorised version $\hat{f}$, that applies to
\textit{segments} of some larger array.  While flattening is useful
for its universal applicability, it has three main problems:

\begin{enumerate}
\item \textit{All} parallelism is exploited, even that which is
  expensive to exploit (perhaps hidden behind branches) and not
  necessary to take full advantage of the hardware.
\item The vectorisation transformation forces all sequential loops to
  the outermost level, thus preventing low-overhead sequential loops
  inside threads.  This is particularly harmful for programs that are
  ``almost flat'', such as a \lstinline{map} whose function simply
  performs a sequential loop.  Flattening would transform this into a
  sequential loop that contains a \lstinline{map}, thus forcing an
  array of intermediate results to be written after every
  \lstinline{map}.  In contrast, the original loop may have been able
  to run using just registers.
\item The structure of the original program is heavily modified,
  destroying much information and rendering optimisations based on
  access pattern information (such as loop tiling) infeasible.
\end{enumerate}

Work is still progressing on adapting and improving the flattening
transformation.  For example, \cite{Keller:2012:VA:2364506.2364512}
shows how to avoid vectorisation in places where it produces only
overhead with little gain in parallelism, particularly addressing
problem (2) above.

Flattening remains the only technique to have demonstrated universal
applicability, and is thus useful as a ``last resort'' for awkward
programs that admit no other solution.  However, many interesting
programs only exhibit limited nested parallelism.  Specifically, they
exhibit only \textit{regular} nested parallelism, which is
significantly easier to map to hardware.

Nested parallelism is regular if the amount of parallelism in its
inner parallel loops is invariant to its outer parallel loops, and
otherwise \textit{irregular}.  For example, the following Futhark
expression contains irregular parallelism:

\begin{lstlisting}
map (\i -> reduce (+) 0 [1...i]) [1...n]
\end{lstlisting}

While this one does not:

\begin{lstlisting}
map (\i -> reduce (+) 0 (map (+i) [1...n])) [1...n]
\end{lstlisting}

In the former program, the inner parallel \lstinline{reduce} operates
on an array containing \lstinline{i} elements, where \lstinline{i} is
bound by the function of the \lstinline{map}.  In the latter case, the
\lstinline{reduce} operates on an array of size \lstinline{n}, where
\lstinline{n} is bound outside the expression.  The parameter
\lstinline{i} is still used, but it does not contribute to the size of
any array, only to their values.  While the Futhark language does
support irregular nested parallelism, as demonstrated above, the
current implementation is not able to exploit it.  Should it prove
necessary, the Futhark compiler could be modified to incorporate the
flattening algorithm as well, but for this thesis, I have focused on
developing implementation techniques that are more limited in scope,
but produce faster code.

The limitation to regular nested parallelism is not as onerous as it
may seem.  First, many interesting problems are naturally regular (see
Chapter~\ref{chap:empirical-validation} for examples).  Second, we can
always \textit{manually} apply the flattening algorithm to our program
to the degree necessary to remove irregular nested parallelism.  This
may require manual inlining and thus breaking modularity.  Third, many
irregular programs can be modified in an ad-hoc fashion to become
regular.  For example, the irregular program shown above can be
rewritten to

\begin{lstlisting}
map (\i -> reduce (+) 0
           (map (\x -> if x > n then 0 else x) [1...n]))
    [1...n]
\end{lstlisting}

But this method is not mechanical---a unique approach is required
based on the algorithm in question, in contrast to flattening, which
is general.

The largest problem with the restriction to only regular parallelism
is that it inhibits modularity.  Consider a function that computes the
sum of the first $n$ positive integers for some $n$:

\begin{lstlisting}
let sum_nats (n: i32): i32 =
  reduce (+) 0 [1...n]
\end{lstlisting}

The type of this function is merely \lstinline{i32 -> i32}, and so we
should be able to map it over an array of type \lstinline{i32}:

\begin{lstlisting}
map sum_nats [1...100]
\end{lstlisting}

However, this gives rise to irregular parallelism, because the
parallelism of the \lstinline{map} inside the definition of
\lstinline{sum_nats} depends on the integer argument to
\lstinline{sum_nats}.  This is not a problem that can be detected
merely by the type of \lstinline{sum_nats}.  It remains future work
to investigate language mechanisms that allows programmers to reason
about the regularity of parallelism in a modular fashion.

The restriction to regular parallelism is an artifact of the current
Futhark implementation, not Futhark as a programming language.
Therefore, if necessary, an implementation could be constructed that
avoids this issie by using full flattening.  However, the performance
advantage of regular nested parallelism still motivates a language
mechanism for modular reasoning.

\section{Why a New Language?}
\label{sec:new-language}

Most of the preceding discussion involves concepts and constructs that
are common to many functional languages.  Indeed, it is frequently
claimed that pure functional programming makes parallel execution
trivial.  Why, then, do I propose an entirely new language, rather
than applying my techniques to existing and reasonably popular
languages such as Haskell, OCaml, or F\#?  What does Futhark have that
these languages lack?

The answer lies in the inverse question: what has been excluded from
Futhark to permit efficient parallel execution?  While all mentioned
languages contain the essentials for data-parallel programming---bulk
operations with pure functions---they also contain features and
promote programming styles that complicates the task of writing a
compiler capable of generating high-performance code for restricted
parallel machines.  This is not a task that needs further
complication.

\subsection{Uniqueness Types}

\subsection{Streaming SOACs}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
