\chapter{Introduction}

This thesis describes the design and implementation of
\textit{Futhark}, a data parallel functional array language.
Futhark is a small programming language that is superficially similar
to established functional languages such as OCaml and Haskell, but
with restrictions and extensions meant to permit compilation into
efficient parallel code.  While this thesis contains techniques that
could be applied in other settings, Futhark has been the overarching
context for our work.  We demonstrate the applicability and efficiency
of the techniques by their application in the Futhark compiler, and
the performance of the resulting code.  Apart from serving as a
vehicle for compiler research, Futhark is also a programming language
that is useful in practice for high-performance parallel programming.

It is the task of the compiler to map the high-level portable
parallelism to low-level parallel code that can be executed by some
machine.  For example, a parallel language may support \textit{nested}
parallelism, while most machines efficiently support only
\textit{flat} parallelism.  One problem is that not all parallelism is
born equal, and the optimal depth of parallelisation depends not just
on the static structure of the program, but also on the problem size
encountered at run-time.  When writing a program in data parallel
language, the program tends to contain a large amount of parallelism;
frequently much more than what is necessary to fully exploit the
target machine.  For example, the program may contain two parallel
loops, where executing the second loop in parallel carries significant
overhead.  If the outer loop contains enough parallel iterations to
fully saturate the machine, then it is better to execute the inner
loop sequentially.  On the other hand, if the outer loop contains
comparatively few iterations, then it may be worth paying the overhead
of also executing the innermost loop in parallel.  This decision
cannot be made at compile-time, and thus the compiler should produce
code for both versions, and pick between them at run-time, based on
the problem size.

Many implementation techniques for data-parallelism have been
developed.  From the field of functional programming, we have
Blelloch's \textit{full flattening
  algorithm}~\cite{blelloch1994implementation}, which shows how to
transform \textit{all} parallelism in program written in a nested
data-parallel language to flat parallelism.  While full flattening is
guaranteed to preserve the asymptotic cost and parallelism of the
program, its practical performance is often poor, because some of the
parallelism may carry heavy overhead to exploit, and is unnecessary in
practise.  An example of this could be parallel code hidden behind
branches.  Furthermore, the code generated by full flattening has an
un-analysable structure, preventing locality-of-reference
optimisations such as loop tiling.

For these reasons, flattening is not used much in practise.  Instead,
we find techniques that tend to exploit only easily accessible
parallelism.  One common approach is to exploit only top-level
parallelism, with nested parallelism exploited only in limited ways,
for example by always mapping it to a specific layer of the parallel
hierarchy of the
machine~\cite{grelck2006sac,DeliteNestedPar,mcdonell2013optimising,Steuwer:2015:GPP:2858949.2784754}.
While the resulting code tends to exhibit little overhead, the lack of
flexibility makes it hard to express algorithms whose natural
expression contains nested parallelism.  This is particularly
problematic when exploiting the inner levels of parallelism is
critical to obtaining performance in pracise.

More powerful techniques exist, for example dynamic techniques for
detecting parallelism~\cite{SpLSC} can in principle exploit also
partial parallelism, but at significant run-time overhead.  Static
analyses such as the polyhedral
model~\cite{PolyhedralOpt,RedPencil,PolyPluto2,chatarasi2015polyhedral}
are able to support some forms of nested parallelism, but are
typically limited to affine code, and their success is not guaranteed
by the language semantics.

In this thesis, we bridge the imperative and functional approaches: we
seek to exploit only that parallelism which is necessary or easily
accessible, and we work on the foundation of an explicitly parallel
language.  We are inspired by the systematic approach of flattening,
but we wish to employ full flattening only as a last resort, when
absolutely all parallelism must be exploited.  Our goal is pragmatic.
We wish to write Futhark programs in a high-level hardware-agnostic
and modular style, using nested parallelism where convenient.  The
compiler should then translate the Futhark programs to low-level GPU
code whose run-time performance rivals that of hand-written code.

\paragraph{First Contribution}

Our key contribution is \textit{moderate flattening}
(\cref{chap:kernel-extraction}).  This algorithm, which is inspired by
both full flattening and loop distribution, is capable of restricting
itself to exploiting only that parallelism which is cheap to access,
or more aggressively exploit more of the available parallelism.
Excess parallelism is turned into efficient sequential code, with
enough structure retained to perform important locality-of-reference
optimisations (\cref{chap:tiling}).  The moderate flattening algorithm
is also capable of improving the degree of available parallelism by
rewriting the program.  For example, loop interchange is used to move
parallel loop levels next to each other, even if they were separated
by a sequential loop in the source program.

The moderate flattening algorithm is capable of data-sensitive
parallelisation, where different parallelisations are constructed for
the same program, and the optimal one picked at run-time.

\paragraph{Second Contribution}

The moderate flattening algorithm requires significant infrastructure
to function.  While full flattening transforms everything into flat
bulk parallel constructs, within which all variables are scalars, we
wish to permit just partial parallelisation.  As a result, some
variables in the sequentialised excess parallelism may be arrays.  The
compiler must have a mechanism for reasoning about their sizes.

One contribution here is a technique for \textit{size inference} on
multidimensional arrays (\cref{chap:size-analysis}) that gives us the
property that for every array in the program, there is a variable (or
constant) in scope that describes the size of each dimension of the
array.  This allows the compiler to reason about the variance of
arrays in a symbolic and simple way.  Prior work tends to either
restrict the language to ensure that all symbolic sizes can be
statically determined~\cite{fish}, or requires extensive annotation by
the programmer~\cite{brady2013idris,bove2009dependent}.  Our technique
for size inference is a pragmatic design that does not restrict the
language (by forbidding for example filtering), but also does not
require any annotations on behalf of the programmer.  We do this by
using a simple form of existential types.

\paragraph{Third Contribution}

We present an extension to existing work on fusion
(\cref{chap:fusion}).  The primary novelty is the creation of new
array combinators that permit a greater degree of fusibility.  Because
of the moderate flattening technique, the fusion algorithm cannot know
whether some piece of code will eventually be parallelised or
sequentialised.  We present constructs that have good fusion
properties, but also allow both the recovery of all parallelism, and
the transformation into efficient sequential code.  These are not
properties provided by prior work on fusion.  We also show how
``horizontal'' fusion can be considered a special case of ``vertical''
(producer/consumer) fusion, given sufficiently powerful combinators,
and can be an enabler of vertical fusion.

\paragraph{Fourth Contribution}

Even in a parallel language, it is sometimes useful to implement
efficient sequential algorithms, often applied in parallel to parts of
a data set.  Most functional parallel languages do not support
in-place updates at all, which hinders the efficient implementation of
such sequential algorithms.  While imperative languages obviously
support in-place updates efficiently, they do not guarantee safety in
the presence of parallelism.

We present a system of uniqueness types (\cref{sec:uniqueness-types}),
and a corresponding formalisation~(\cref{sec:uniqueness-formalism}),
that permits a restricted form of in-place updates, that provides the
cost guarantees without compromising functional purity or parallel
semantics.  While more powerful uniqueness type
systems~\cite{clean-uniqueness-types}, and affine and linear
types~\cite{Tov:2011:PAT:1926385.1926436,Fahndrich:2002:AFP:543552.512532}
are known, ours is the first application that directly addresses
\texttt{map}-style parallel constructs, and shows how in-place updates
can be supported without making evaluation order observable.  Our
design for in-place updates is similar to static single assignment
form (SSA), and maintains explicit data dependencies in the compiler
IR.  We show that the addition of uniqueness types does not overly
burden the implementation of compiler optimisations.

\paragraph{Fifth Contribution}

We demonstrate the GPU performance of code generated by the Futhark
compiler on a a set of $23$ nontrivial problems adapted from published
benchmark suites.  $17$ of the benchmarks are compared to hand-written
implementations, but we also include six programs ported from
Accelerate~\cite{mcdonell2013optimising}, a mature Haskell library for
parallel array programming.  Our results show that the Futhark
compiler generates code that performs comparably with hand-written
code in most cases, and generally outperforms Accelerate.

\paragraph{Sixth Contribution}

The Futhark compiler is implemented with approximately $45,000$ lines
of Haskell, and is available under a free software license at

\centerline{\url{https://github.com/diku-dk/futhark/}}

The compiler and language is sufficiently documented for usage by
third parties.  The compiler performs several operations whose
implementations took significant engineering effort, but are not
covered in this thesis due to lack of scientific contribution.  These
include defunctorisation for an ML-style higher order module system,
hoisting, variance/data-dependency analysis, memory management, memory
expansion, OpenCL code generation, and a plethora of classical
optimisations.

We believe that the Futhark compiler can serve as a starting point for
future research in optimising compilers for explicitly parallel
languages, as well as a tool for implementing data-parallel
algorithms.

\paragraph{Closing}

At a high level, we argue that techniques from imperative compilers
can be lifted to a functional and explicitly parallel setting, where
analyses are more scalable.  We prove that using these techniques, a
compiler can be written that generates highly performant code for
non-contrived programs.  We demonstrate the resulting performance
compared to hand-written code (\cref{chap:empirical-validation}),
where Futhark in most cases approaches or even exceeds the performance
of hand-written low-level code.

Most of this work has previously been published in FHPC
2013~\cite{henriksen2013t2}, FHPC 2014~\cite{henriksen2014size}, ARRAY
2016~\cite{Futhark:redomap,} and PLDI
2017~\cite{henriksen2017futhark}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
