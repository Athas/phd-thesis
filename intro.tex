\chapter{Introduction}

TODO:

\begin{itemize}
\item scope and philosophical level of how our approach is placed compared to imperative approaches

\item show the vision

\item futhark source language examples and introduction

\item show how techniques from imperative compilers become easier in a functional setting

\item talk about all kinds of promising techniques, followed by a compiler that can actually do some of them.
\end{itemize}

\textit{Ahem...}

In physics, the speed of light, denoted $c$, is the ultimate speed
limit of the universe.  Likewise in programming, ``as fast as C'' is
often been used as an indication that some programming language is as
fast as it can possibly be.  In theory it makes no sense to say that a
given programming language is ``slow'' or ``fast'', as these are
merely properties of a particular implementation of the programming
language running on a specific computer.  But in practise, it is clear
that the design of a programming language has an overwhelming
influence on the ease with which a performant implementation can be
constructed.

For decades, the design of languages such as C has permitted the
implementation of compilers that generate efficient code.  One
important reason is that the abstract machine model underlying C, a
sequential random-access register machine, can be mapped easily to
mainstream computers with little overhead.  This means that a
programmer can write code in C and have a reasonable idea of the
performance of the resulting code.  It also means that even a naive
non-optimising C compiler can generate code with good performance,
although it may require somewhat more care from the C programmer.
Indeed, C has often been described as ``portable assembly
code''.\footnote{I will using C as the main point of reference in the
  following sections, but the problems I point out are common to any
  sequential language.}

In contrast, languages whose abstract model is more different from
physical machines, so-called ``high-level languages'', cannot be as
easily mapped to real machines.  Compilers must invest considerable
effort in mapping e.g. the call-by-need lambda calculus of Haskell to
the sequential register machine~\cite{jones1992implementing}.  Even
after decades of work, high-level languages struggle to catch up to
the performance of C on the sequential random-accesss register
machine.

Of course, a cursory study of modern hardware designs show that the
sequential random-access register machine is a lie.  The abstract
model of C may match a 70s minicomputer reasonably well, but it is
increasingly diverging from how modern computers are constructed.
This is not despite lack of effort on behalf of the hardware
designers: due to the massive popularity (and therefore economic
significance) of C-like languages, manufacturers have attempted to
prolong the lie of the sequential random-access machine for as long as
possible.

\section{Physical Challenges to Improving CPU Performance}

Unfortunately, while C may be exceeded (as I hope to show in this
thesis), $c$ is a harder nut to crack.  The machine on which I am
typing contains a CPU with a clock frequency of $2.3GHz$,
corresponding to a clock cycle taking $0.43ns$.  Given that $c$ is
approximately $3\cdot10^{8}m/s$, light moves approximately $13cm$ in
the the time the CPU takes to perform a clock cycle, thus physically
limiting the distance a single piece of information can travel in a
single cycle.

Another physical limitation is power dissipation: roughly, the power
usage of the transistors in a processor is roughly proportional to the
square of its clock frequency.  This prevents an increase in clock
frequency unless we can compensate with an increase in the efficiency
of the transistors.  For a sequential machine that executes
instructions in exactly the order they are given, the only way to
speed up execution of a program is to increase the rate at which
instructions are executed.  If we can double the rate at which
instructions are processed, we in effect halve the time it takes to
execute some program (assuming no memory bottlenecks, which I will
discuss later).

In the popular vernacular, \textit{Moore's Law} is typically rendered
as ``computers double in speed every two years''.  But actually, the
law states that \textit{transistor density} doubles every two years
(implying that transistors become smaller).  Moore's Law does not
state that the enlargened transistor budget translates
straightforwardly to improved performance, although that was indeed
the case for several decades.  This is due to another law,
\textit{Dennard scaling}, which roughly states that as transistors get
smaller, their power density stays constant.  Taken together, Moore's
Law and Dennard scaling rougly say that we can put ever more
transistors into the same physical size, and with the same power usage
(and thus heat generation).

The reduction in power usage per transistor granted by Dennard scaling
permitted a straightforward increase in CPU clock frequency.  Roughly,
every time we cut the power consumption of a transistor in half via
shrinkage, we can increase the clock frequency by 50\%.  For a
sequential computer, this translates into a 50\% performance increase.
We can partially circumvent the limits of $c$ by using techniques such
as \textit{pipelining}.  While it is clear that Moore's Law will
eventually stop, or else transistors would eventually be smaller than
a single atom, this is not the issue that has hindered the lie of the
sequential machine.

The problem is that Dennard scaling began to break down around 2006.
Physical properties of the circuit material leads to increased current
leakage at small sizes, causing the chip to heat up.  Simply pushing
up the clock frequency is no longer viable.  Instead, chip
manufacturers use the transistor budget on increasing the amount of
work that can be done in a clock cycle through various means:

\begin{description}
\item[Increasing the size of caches.] One significant problem with
  modern computer designs is that processors are significantly faster
  than the memory from which they get their data.  As a rule of thumb,
  accessing main memory incurs a latency of $100ns$ - likely hundreds
  of clock cycles on a current CPU.  This so-called \textit{memory
    wall} can be circumvented by the addition of caches that store
  small subsets of the larger memory.  Heuristics, usually based on
  temporal locality, are useed to determine which parts of the larger
  memory are stored in the caches.  High-performance CPUs can use
  several layers caches, each layer being larger and slower than the
  former.
\item[Inferring instruction-level parallelism.]  While the semantics
  of a sequential CPU is that instructions are executed one after
  another, it is often the case that two instructions to not have
  dependencies on one another, and thus can be executed in parallel.
  CPUs even perform \textit{out-of-order} execution where later
  instructions are executed before earlier ones, if the latter
  instruction has no dependence on the latter.
\item[Explicit parallelism at the hardware level.]  The two former
  techniques try to masquerade the fact that the sequential machine
  model is increasingly physically untenable.  Another approach is to
  explicitly provide programmers with hardware support for parallel
  execution.  This most famously takes the form of adding additional
  CPU cores, each of which processes a sequential stream of
  instructions, but an equally important technique is to add
  \textit{vector instructions} that operate on entire vectors of data,
  rather than single values.  As an example, the newest Intel AVX-512
  vector instruction set provides instructions that operate on entire
  512-bit vectors (for example, 16 single precision floating point
  values).  Such instructions dramatically increase the amount of work
  that is done in a single clock cycle.
\end{description}

These techniques are all based on merely extending and improving the
classic CPU design.  While code may have to be re-written to take
advantage of certain new hardware features (such as multiple cores or
vector instructions), the programming experience is not too dissimilar
from programming sequential machines.  This is not necessarily a bad
thing: sequential machines are easy to reason about, generous in their
support of complex control flow, and have a significant existing base
of programmer experience.

However, the notion that C is ``portable assembly'' begins to crack
noticeably even at this point.  C itself has no built-in notion of
multi-threaded programming, nor of vector operations, and thus vendors
have to provide new programming APIs (or language extensions) for
programmers to take advantage of the new (performance-critical)
machine features.  Alternatively, C compilers must perform significant
static analysis of the sequential C code to find opportunities for
vectorisation or multithreading.  In essence, the compiler must
reverse engineer the sequential statements written by the programmer
to reconstruct whatever parallelism may be present in the original
algorithm.

The problem is that C was carefully designed for a particularly class
of machines; a machine no longer resembled by modern high-performance
computers.  Thus, the impedance mismatch between C and hardware
continues to grow, with little sign of stopping.  This mismatch
becomes acute when we move away from the comparatively benign world of
multicore CPUs and into the realm of massively parallel processors.

\section{Massively Parallel Machines}

To a large extent, mainstream CPUs favour programming convenience,
familiarity, and backwards compatibility over raw performance.  We
must look elsewhere for examples of the kind of hardware that could be
built if we were willing to sacrifice traditional sequential notions
of sequential programming.

For as long as there has been computers, there have been parallel
computers.  Indeed, if we think back to the very earliest computers,
the human ones made of flesh and blood, parallelism was the
\textit{only} way to speed up a computation.  You were unlikely to
make a single human computer much faster through training, but you
could always pack more of them into a room.  However, with the rise of
electrical computers, and in particular the speed with which
sequential performance improved, parallelism dwindled into the
background.  Certainly, you could have two computers cooperate on
solving some problem, but the programming difficulties involved made
it more palatable to simply wait for a faster computer to enter the
market.  Only the largest computational problems were worth the pain
of parallel programming.\footnote{\textit{Concurrency} was alive and
  well, however, due to its significant importance in operating
  systems and multiprocessing.  But most concurrent programs were
  executed on sequential machines, with the illusion of concurrency
  formed through multiplexing.  Many of the techniques developed for
  concurrent programming are also applicable to parallel programming,
  although concurrency tends to implicitly focus on correctness over
  performance on some specific machine.}

Several interesting parallel computers were designed for
\textit{high-performance computing} (HPC) applications.  One of the
earliest and most influential was the Cray-1 from 1976, which was the
first vector processor.  Only about 100 were sold, but this is a large
number for a high-end supercomputer.  A different design was the CM-1
from 1985, which was based on a computer containing a large number of
simple 1-bit microprocessors.  The CM-1 proved difficult to program
and was not a success in the market, but was the first example of a
\textit{massively parallel machine}.

In the 90s, consumer demand for increasing visual fidelity in video
games led to the rise of the \textit{graphics processing unit} (GPU),
for accelerating graphical operations that would be too slow if
executed on the CPU.  Initially, GPUs were special-purpose
non-programmable processors that could only perform fixed graphical
operations.  Over time, the need for more flexibility in graphical
effects lead to the development of programmable \textit{pixel
  shaders}, first seen in the NVIDIA GeForce 3 in 2000.  Roughly,
pixel shaders allowed an effect to be applied to every pixel of an
image - for example, looking at every pixel and add a reflection
effect to those that represent water.  Graphical operations such as
these tend to be inherently parallel, and as a result, GPU hardware
evolved to support efficient execution of programs with very simple
control flow, but a massive amount of fine-grained parallelism.
Eventually, GPU manufacturers started providing APIs that allowed
programmers to exploit the parallel computational power of the
now-mainstream GPUs even for non-graphics workloads, so-called
\textit{general-purpose GPU programming} (GPGPU).  The most popular
such APIs are CUDA~\cite{cuda} from NVIDIA, and
OpenCL~\cite{Stone:2010:OPP:622179.1803953}.

GPUs are not the only massively parallel machines in use.  However,
their great success, their availability, the difficulty in programming
them, and their potential compute power makes them excellent objects
of study for researchers of parallel programming models.  In this
work, I focus on the details of GPUs over other parallel machines.
However, a central thesis of the work is that modern hardware is too
complicated and too diverse for low-level programming by hand to be
viable.  I will introduce a high-level hardware-agnostic programming
model, and describe its efficient mapping to GPUs.  The implication is
that if the model can be mapped efficiently to a platform as
restricted as GPUs, it can probably also be mapped to other parallel
platforms, such as multicore CPUs or FPGAs.

\subsection{Basic Properties of GPUs}

The performance characterictics and programming model of modern GPUs
is covered in greater detail on~\ref{chap:hardware}, but a basic
introduction is given here, in order to give an idea of the
difficulties involved in retrofitting sequential languages for GPU
execution.

GPUs derive their performance from an execution model called
\textit{single instruction multiple thread} (SIMT), which is very
similar to the \textit{single instruction multiple data} model.
Roughly, threads are not fully independent, but grouped into bundles
that all execute the same operations on different parts of a large
data set.  For example, on an NVIDIA GPU, threads are bundled into
\textit{warps}, of 32 threads each, that execute in lockstep, and
which form the unit of scheduling.  This execution is highly suited
for dense and regular computations, such as the ones found in linear
algebra, but less so for irregular and branch-heavy computations such
as graph algorithms.

A GPU is still subject to the laws of physics, and there is therefore
a significant latency between issuing a memory read, and actually
receiving the requested data (the memory wall).  On a CPU, a hierarchy
of caches is used to decrease the latency, but a GPU uses aggressive
\textit{simultaneous multithreading}, where a thread that is waiting
for a memory operation to complete is de-scheduled and another thread
run in its place.  This scheduling is done entirely in hardware, and
thus does not carry the usual overhead of context switches.  This
scheduling is not done on each thread in isolation, but on entire
32-thread warps.  Thus, while a GPU may be only have enough
computational units (such as ALUs) to execute a few thousand parallel
instructions per clock cycle, tens of thousands of threads may be
necessary to avoid the computational units being idle while waiting
for memory operations to finish.  As a result of this design, memory
can also be optimised for very high bandwidth at the expense of
latency, and it is not unusual for GPU memory buses to support
bandwidth in excess of $300GiB/s$ (although as we shall discuss in
Chapter~\ref{chap:hardware}, specific access patterns must be followed
to obtain peak performance).

GPUs have many limitations that hinder traditional programming
techniques and languages:

\begin{itemize}
\item GPUs function as \textit{co-processors}, where code is
  explicitly uploaded and invoked.  A GPU program is typically called
  a \textit{GPU kernel}, or just \textit{kernel}.
\item The amount of threads necessary implies that each thread can use
  only relatively few registers and little memory (including stack
  space).
\item The lockstep execution of warps, as well as the very small
  per-thread stack size, prevents function pointers and recursion.
\item GPUs cannot directly access CPU memory.\footnote{This is changing with the newer generation of GPUs.}
\item Memory allocation is typically not possible while executing a
  GPU program.  All necessary memory must be pre-computed before
  starting GPU execution.
\item Generally no support for interrupts or signals.
\end{itemize}

The mismatch between the abstract machine model of C and GPU
architectures is clear.  While current GPU programming APIs do in fact
use a restricted subset of C (or C++ in the case of CUDA) for
expressing the per-thread code, it is the programmmer's responsibility
to orchestrate their execution.  This involves organising
communication between the tens of thousands of threads that are needed
to saturate the hardware---a tall order, and the compiler cannot help.

\section{A Parallel Programming Language}

We need a better programming model; one that does not have as
fundamental a mismatch between its execution model and parallel
hardware.  This does not mean we need a low-level GPU-specific model.
Rather, we should go the other way and stop \textit{overspecifying}
things like iteration order and sequencing, unless we really
\textit{need} something to execute in some specific order.  In a C
program, everything is required to execute in exactly the order
written by the programmer, and the C compiler must perform significant
work to figure out which of the sequencing restrictions are essential
and which are accidental.

A more reasonable programming model is one based on \textit{bulk
  operations}, where we program in terms of large-scale
transformations of collections of data.  Suppose we wish to increment
every element in a vector by $2$.  The functional programming
tradition comes with an established vocabulary of such bulk operations
which we can use as inspiration.  For this task, we can use the
\lstinline{map} construct, which takes a function
$\alpha~\rightarrow\beta$ and a collection of values of type $\alpha$,
and produces a collection of values of type $\beta$:

\begin{lstlisting}
map (\x -> x + 2) xs
\end{lstlisting}

The above is a valid expression in the Futhark programming language.
We use the notation \lstinline{(\x -> ...)}, taken from Haskell, to
express an anonymous function with a parameter \lstinline{x}.  This
expression does not specify \textit{how} the computation is to be
carried out, and it does not imply any accidental ordering
constraints.  Indeed, the only restriction is that this expression can
only be evaluated after the expression that produces \lstinline{xs}.

In this section, and those that follow, Futhark will be used to
demonstrate the qualities of parallel programming models.  Futhark is
by no means the first parallel programming language, nor even the
first parallel functional language.  That title likely belongs to the
venerable APL, which was first described in 1962~\cite{iversonbook}.
Futhark is not even the first parallel language in a
$\lambda$-calculus style, as it is predated by
NESL~\cite{BlellochCACM96NESL} by some twenty years.  Indeed, as we
shall see, Futhark is not even the most expressive such language.
Futhark has more similarities than differences from other parallel
functional languages.  The main contributions of this thesis are the
\textit{implementation techniques} that have been developed for
Futhark---its efficient mapping to GPU hardware---as well as those
bits of its language design that enable said implementation.  It seems
likely that the implementation techniques could be applied to other
functional languages of roughly the same design.

Let us return to the \lstinline{map} expression:

\begin{lstlisting}
map (\x -> x + 2) xs
\end{lstlisting}

The style of parallelism used by Futhark is termed \textit{explicit
  data parallelism}.  It is \textit{explicit} because the user is
required to use special constructs (here, \lstinline{map}) to inform
the compiler of where the parallelism is located, and it is
\textit{data parallel} because the same operation is applied to
different pieces of data.  In contrast, thread-based programming is
based on \textit{task parallelism}, where different threads perform
different operations on different pieces of data (or a shared piece of
data, if one enjoys debugging race conditions).  Task parallelism does
necessarily imply low-level and racy code with manual synchronisation
and message passing, but can be given safe structure through
e.g. futures or fork/join patterns.

One important property of functional data parallelism is that the
semantics are sequential.  The program can be understood entirely as a
serial sequence of expressions evaluating to values, with parallel
execution (if any) not affecting the result in any way.  This
decoupling of semantics and operations is typical of functional
languages, and is key to enabling aggressive automatic transformation
by a compiler.  It is also easier for programmers to reason
sequentially than in parallel.  It is still important to the
programmer that an operation such as \lstinline{map} is, however,
operationally parallel can be described through parallel cost models.
One such cost model was described and subsequently proven
implementable for the data parallel language
NESL~\cite{Blelloch:1996:PTS:232627.232650}.  While a formal cost
model for Futhark it outside the scope of this thesis, the similarity
of Futhark to NESL suggests that a similar approach would be viable.
Instead, we use the intuitive notion that certain constructs (such as
\lstinline{map}) may be executed in parallel.  However, it is not
always efficient to translate all \textit{potential} parallelism into
\textit{realised parallelism} on a concrete machine.

\subsection{Efficient Sequentialisation}

In the literature, a \textit{parallelising compiler} is a compiler
that takes as input a program written in some sequential language,
typically C or Fortran, and attempts to automatically deduce
(sometimes with the help of programmer-given annotations) which loops
are parallel, and how best to exploit the available parallelism.  A
large variety of techniques exist, ranging from sophisticated static
approaches based on loop index analysis~\cite{PolyhedralOpt}, to
speculative execution that assumes all loops are parallel, and
dynamically falls back to sequential execution of the assumption fails
at runtime~\cite{SpLSC}.  The Futhark compiler is \textit{not} such a
parallelising compiler.  Instead, we assume that the programmer has
already made all parallelism explicit via constructs such as
\lstinline{map} (and others we will cover).  In this style of
programming, it is likely that the program contains \textit{excess
  parallelism}, that is, more parallelism than the machine needs.  As
almost all parallelism comes with a cost in terms of overhead, one of
the main challenges of the Futhark compilers is to figure how much of
this parallelism to actually take advantage of, and how much to turn
into low-overhead sequential code via \textit{efficient
  sequentialisation}.  It is therefore more correct to say that the
Futhark compiler is a \textit{sequentialising compiler}.

For example, let us consider the \lstinline{reduce} construct, which
is used for transforming an array of elements of type $\alpha$ into a
single element of type $\alpha$:

\begin{lstlisting}
reduce (\x y -> x + y) 0 xs
\end{lstlisting}

We require that the functional argument is an associative function,
and that the second element is a neutral element for that function.
The conventional way to illustrate a parallel reduction is via a tree,
as on Figure~\ref{fig:tree-summation}.  To reduce an $n$-element array
$[x_{1},\ldots,x_{n}]$ using operator $\oplus$, we launch $n/2$
threads, with thread $i$ computing $x_{2i}\oplus{}x_{2i+1}$.  The
initial $n$ elements are thus reduced to $n/2$ elements.  The process
is repeated until just a single value is left--the final result of the
reduction.  We perform $O(\log(n))$ partial reductions, each of which
is perfectly parallel, resulting in a work depth of
$O(\log(n))$.

Tree reduction is optimal on an idealised perfectly parallel machine,
but on real hardware, such as GPUs, it is inefficient.  The
inefficiency is caused by exposing more parallelism than needed to
fully exploit the hardware.  The excess parallelism means we pay an
unnecessary overhead due to communication cost between threads.  For a
summation, the overhead of communicating intermediate results between
processors significantly dominates the cost of a single addition.
Efficient parallel execution relies on exposing as much parallelism as
is needed to saturate the machine, but no more.

\begin{figure*}
  \centering
  Here goes a graph.
  \caption{Summation as a tree reduction.}
  \label{fig:tree-summation}
\end{figure*}

\begin{figure*}
  \centering
  Here goes a graph.
  \caption{Summation as a chunked tree reduction.}
  \label{fig:chunked-summation}
\end{figure*}

The optimal amount of parallelism depends on the hardware and exact
form of the reduction, but suppose that parallel $k$ threads are
sufficient.  Then, instead of spawning a number of threads dependent
on the input size $n$, we always spawn $k$ threads.  Each thread
sequentially reduces a chunk of the input consisting of $\frac{n}{k}$
elements, producing one intermediate result per thread.  We then
launch a second reduction over all these intermediate results.  This
second reduction can also be done in parallel, or can be sequential if
$k$ is sufficiently small.

Efficient sequentialisation is not a single implementation technique,
but a general implementation philosophy, which gives rise to various
techniques and design choices.  It is a principle that I shall often
return to during this thesis, as it has proven critical for executing
Futhark efficiently on real hardware.

\subsection{Modular Programming}

One of the most important goals for most programming languages is to
support modular and abstract programming.  A modular program is
composed of nominally independent subcomponents, which are composed to
form a full program.  The simplest feature that supports modular
programming is perhaps the \textit{procedure}.  Almost all programming
languages support the subdivision of a program into procedures,
although most languages also support higher-level abstractions.  The
most important property of a procedure is that it can be understood in
terms of its specification, rather than its implementation.  This
simplifies reasoning by abstracting away irrelevant details.

As a purely functional language, procedures in Futhark are called
\textit{functions}.  To support a programming style based on the
writing of small, reusable components, it is important that there is
little run-time overhead to the use of functions.  Function inlining
is a well established technique to remove the overhead of function
calls, although at the cost of an increase in code size.  Another
useful property of inlining is that it enables further optimisation,
as a opaque function call is replaced with the body of the function,
which may be subject to simplification in the context of the caller.
While wide-spread techniques such as copy propagation, constant
folding, and dead code removal remain useful in a data-parallel such
as Futhark, other, more sophisticated, transformations are also
important.

One such transformation is \textit{loop fusion}.  Let us consider two
Futhark functions on arrays:

\begin{lstlisting}
let arr_abs (xs: []i32) = map i32.abs xs

let arr_incr (xs: []i32) = map (+1) xs
\end{lstlisting}

In Futhark, both functions and local bindings are defined using the
\lstinline{let} keyword.  These functions both take a single
parameter, \lstinline{xs} of type \lstinline{[]i32}.  We write the
type of arrays containing elements of type \lstinline{t} as
\lstinline{[]t}.  The function \lstinline{arr_abs} applies the
function \lstinline{i32.abs} (absolute value of a 32-bit integer) to
every element of the input array.  The function \lstinline{arr_scale}
increases every element of the input by \lstinline{1}.  We use a
shorthand for the function: \lstinline{(+1)} is equivalent to
\lstinline{\x -> x + 1}, similarly to the \textit{operator slices} of
Haskell.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
