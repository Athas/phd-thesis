\chapter{Introduction}

% \begin{itemize}
% \item scope and philosophical level of how our approach is placed compared to imperative approaches

% \item show the vision

% \item futhark source language examples and introduction

% \item show how techniques from imperative compilers become easier in a functional setting

% \item talk about all kinds of promising techniques, followed by a compiler that can actually do some of them.

% \item important contributions:

%   \begin{itemize}
%   \item moderate flattening/data-sensitive
%   \item bridging pragmatic/imperative (loop distribution/common-case
%     parallelism) and functional approaches (flattening)
%   \item blelloch: all parallelism (but inefficient)
%   \item futhark: some top-level parallelism + loc of ref opts
%   \item challenging to moderate blelloch's algorithm
%   \item nested parallelism
%   \item simple in-place updates in the presence of parallelism
%   \item in-place updates similar to SSA
%   \item generality; could apply to non-Futhark
%   \item All other optimisations/analyses (e.g. size analysis)
%     motivated to moderate flattening
%   \item Must defend every contribution
%   \item size analysis: a pragmatic way of inferring
%     array shapes in a way that does not require user annotations; 'fish' automatically infers but restricts the language, other things are general but requires extensive annotations (idris).
%   \item extension of fusion: must not compromise parallelism or efficient sequentialisation; new operators.  Compare to Accelerate, SaC, Lift, polyhedral
%   \item uniqueness type formalisation: linear types, clean.  Applied to parallel constructs.
%   \item closing: show that techniques from imperative compilers can be lifted to a functional setting, which makes analyses more scalable
%   \end{itemize}

% \item forward pointers
% \end{itemize}

This thesis describes the design and implementation of
\textit{Futhark}, a data parallel functional array language.
Futhark is a small programming language that is superficially similar
to established functional languages such as OCaml and Haskell, but
with restrictions and extensions meant to permit compilation into
efficient parallel code.  While this thesis contains techniques that
could be applied in other settings, Futhark has been the overarching
context for our work.  We demonstrate the applicability and efficiency
of the techniques by their application in the Futhark compiler, and
the performance of the resulting code.  Apart from serving as a
vehicle for compiler research, Futhark is also a programming language
that is useful in practice for high-performance parallel programming.

It is the task of the compiler to map the high-level portable
parallelism to low-level parallel code that can be executed by some
machine.  For example, a parallel language may support \textit{nested}
parallelism, while most machines efficiently support only
\textit{flat} parallelism.  One problem is that not all parallelism is
born equal, and the optimal depth of parallelisation depends not just
on the static structure of the program, but also on the problem size
encountered at run-time.  When writing a program in data parallel
language, the program tends to contain a large amount of parallelism;
frequently much more than what is necessary to fully exploit the
target machine.  For example, the program may contain two parallel
loops, where executing the second loop in parallel carries significant
overhead.  If the outer loop contains enough parallel iterations to
fully saturate the machine, then it is better to execute the inner
loop sequentially.  On the other hand, if the outer loop contains
comparatively few iterations, then it may be worth paying the overhead
of also executing the innermost loop in parallel.  This decision
cannot be made at compile-time, and thus the compiler should produce
code for both versions, and pick between them at run-time, based on
the problem size.

From the field of functional programming, we have Blelloch's
\textit{full flattening algorithm}~\cite{blelloch1994implementation},
which shows how to transform \textit{all} parallelism in program
written in a nested data-parallel language to flat parallelism.  While
full flattening is guaranteed to preserve the asymptotic cost and
parallelism of the program, its practical performance is often poor,
because some of the parallelism may carry heavy overhead to exploit,
and is unnecessary in practise.  An example of this could be parallel
code hidden behind branches.  Furthermore, the code generated by full
flattening has an un-analysable structure, preventing
locality-of-reference optimisations such as loop tiling.

Outside of flattening, we find techniques that tend to exploit only
easily accessible top-level parallelism.  Typically only
``embarassingly parallel'' loops where every iteration is independent,
corresponding to a ``map'' operation in functional languages, are
parallelised.  Nested parallelism tends to be exploited only in
limited ways, for example by always mapping it to a specific layer of
the parallel hierarchy of the machine.  While the resulting code tends
to exhibit little overhead, the programming model lacks flexibility.
More powerful techniques exist~\cite{PolyhedralOpt}, but they tend to
not provide guarantees about extracted parallelism, as they rely on
intensive and complicated analyses to detect the parallelism in the
first place.

In this thesis, we bridge the imperative and functional approaches: we
seek to exploit only that parallelism which is necessary or easily
accessible, and we work on the foundation of an explicitly parallel
language.  We are inspired by the systematic approach of flattening,
but we wish to employ full flattening only as a last resort, when
absolutely all parallelism must be exploited.  Our goal is pragmatic.
We wish to write Futhark programs in a high-level hardware-agnostic
and modular style, using nested parallelism where convenient.  The
compiler should then translate the Futhark programs to low-level GPU
code whose run-time performance rivals that of hand-written code.

\paragraph{First Contribution}

Our key contribution is \textit{moderate flattening}
(\cref{chap:kernel-extraction}).  This algorithm, which is inspired by
both full flattening and loop distribution, is capable of restricting
itself to exploiting only that parallelism which is cheap to access,
or more aggressively exploit more of the available parallelism.
Excess parallelism is turned into efficient sequential code, with
enough structure retained to perform important locality-of-reference
optimisations (\cref{chap:tiling}).  The moderate flattening algorithm
is also capable of improving the degree of available parallelism by
rewriting the program.  For example, loop interchange is used to move
parallel loop levels next to each other, even if they were separated
by a sequential loop in the source program.

The moderate flattening algorithm is capable of data-sensitive
parallelisation, where different parallelisations are constructed for
the same program, and the optimal one picked at run-time.

\paragraph{Second Contribution}

The moderate flattening algorithm requires significant infrastructure
to function.  While full flattening transforms everything into flat
bulk parallel constructs, within which all variables are scalars, we
wish to permit just partial parallelisation.  As a result, some
variables in the sequentialised excess parallelism may be arrays.  The
compiler must have a mechanism for reasoning about their sizes.

One contribution here is a technique for \textit{size inference} on
multidimensional arrays (\cref{chap:size-analysis}) that gives us the
property that for every array in the program, there is a variable (or
constant) in scope that describes the size of each dimension of the
array.  This allows the compiler to reason about the variance of
arrays in a symbolic and simple way.  Prior work tends to either
restrict the language to ensure that all symbolic sizes can be
statically determined~\cite{fish}, or requires extensive annotation by
the programmer~\cite{brady2013idris,bove2009dependent}.  Our technique
for size inference is a pragmatic design that does not restrict the
language (by forbidding for example filtering), but also does not
require any annotations on behalf of the programmer.  We do this by
using a simple form of existential types.

\paragraph{Third Contribution}

We present an extension to existing work on fusion
(\cref{chap:fusion}).  The primary novelty is the creation of new
array combinators that permit a greater degree of fusibility.  Because
of the moderate flattening technique, the fusion algorithm cannot know
whether some piece of code will eventually be parallelised or
sequentialised.  We present constructs that have good fusion
properties, but also allow both the recovery of all parallelism, and
the transformation into efficient sequential code.  These are not
properties provided by prior work on fusion.  We also show how
``horizontal'' fusion can be considered a special case of ``vertical''
(producer/consumer) fusion, given sufficiently powerful combinators,
and can be an enabler of vertical fusion.

\paragraph{Fourth Contribution}

Even in a parallel language, it is sometimes useful to implement
efficient sequential algorithms, often applied in parallel to parts of
a data set.  Most functional parallel languages do not support
in-place updates at all, which hinders the efficient implementation of
such sequential algorithms.  While imperative languages obviously
support in-place updates efficiently, they do not guarantee safety in
the presence of parallelism.

We present a system of uniqueness types (\cref{sec:uniqueness-types}),
and a corresponding formalisation~(\cref{sec:uniqueness-formalism}),
that permits a restricted form of in-place updates, that provides the
cost guarantees without compromising functional purity or parallel
semantics.  While more powerful uniqueness type
systems~\cite{clean-uniqueness-types}, and affine and linear
types~\cite{Tov:2011:PAT:1926385.1926436,Fahndrich:2002:AFP:543552.512532}
are known, ours is the first application that directly addresses
\texttt{map}-style parallel constructs, and shows how in-place updates
can be supported without making evaluation order observable.  Our
design for in-place updates is similar to static single assignment
form (SSA), and maintains explicit data dependencies in the compiler
IR.  We show that the addition of uniqueness types does not overly
burden the implementation of compiler optimisations.

\paragraph{Fifth Contribution}

We demonstrate the GPU performance of code generated by the Futhark
compiler on a a set of $23$ nontrivial problems adapted from published
benchmark suites.  $17$ of the benchmarks are compared to hand-written
implementations, but we also include six programs ported from
Accelerate~\cite{mcdonell2013optimising}, a mature Haskell library for
parallel array programming.  Our results show that the Futhark
compiler generates code that performs comparably with hand-written
code in most cases, and generally outperforms Accelerate.

\paragraph{Sixth Contribution}

The Futhark compiler is implemented with approximately $45,000$ lines
of Haskell, and is available under a free software license at

\centerline{\url{https://github.com/diku-dk/futhark/}}

The compiler and language is sufficiently documented for usage by
third parties.  The compiler performs several operations whose
implementations took significant engineering effort, but are not
covered in this thesis due to lack of scientific contribution.  These
include defunctorisation for an ML-style higher order module system,
hoisting, variance/data-dependency analysis, memory management, memory
expansion, OpenCL code generation, and a plethora of classical
optimisations.

We believe that the Futhark compiler can serve as a starting point for
future research in optimising compilers for explicitly parallel
languages, as well as a tool for implementing data-parallel
algorithms.

\paragraph{Closing}

At a high level, we argue that techniques from imperative compilers
can be lifted to a functional and explicitly parallel setting, where
analyses are more scalable.  We prove that using these techniques, a
compiler can be written that generates highly performant code for
non-contrived programs.  We demonstrate the resulting performance
compared to hand-written code (\cref{chap:empirical-validation}),
where Futhark in most cases approaches or even exceeds the performance
of hand-written low-level code.

Most of this work has previously been published in FHPC
2013~\cite{henriksen2013t2}, FHPC 2014~\cite{henriksen2014size}, FHPC
2016~\cite{Futhark:redomap,} and PLDI
2017~\cite{henriksen2017futhark}.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
