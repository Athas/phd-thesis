\chapter{Optimising for Locality of Reference}
\label{chap:tiling}

While GPUs possess impressively fast memory, the ratio of computation
to memory speed is even more lopsided than on a CPU.  As a
consequence, accessing memory efficiently is of critical importance to
obtaining high performance.  Furthermore, while CPUs have multiple
layers of automatic caching to help mitigate the effect of the memory
wall, GPUs typically have only a single very small level-1 cache.
This chapter presents two novel optimisations that improves the memory
access patterns of Futhark programs.  \Cref{sec:automatic-coalescing}
shows how arrays traversed in a kernel can have their representation
in memory modified to ensure that the traversal is efficient.  This
form of optimisation, which changes not just the \textit{code}, but
also the \textit{data}, is unusual for a compiler.
\Cref{sec:automatic-tiling} shows a technique for loop tiling that
uses the local memory of the GPU as a cache to minimise the amount of
traffic to global memory.  While loop tiling is an established
optimisation technique, our approach can handle even indirect
accesses.  These are not supported by conventional techniques for loop
tiling, including polyhedral approaches such as the ones discussed in
\cite{chatarasi2015polyhedral}.

To express the optimisations we have to extend the core Futhark IR,
yet again, with a few more operations.  These are shown on
\cref{fig:lor-constructs}.

\begin{figure}[hbt]
  \begin{tabular}{lcl}
    \emph{op} & & \textrm{TySch}(\emph{op}) \\ \hline
    \kw{manifest} ($c_{1}, \ldots, c_{n}$) & : & $\forall\nseq{d}{n}\alpha.[d_{1}]\cdots[d_{n}]\alpha\rightarrow[d_1]\cdots[d_n]\alpha$ \\
    \kw{kernel} & : & $\forall \nseq{\alpha}{m}.(\nseq{(d_{i}:\texttt{i32})}{n})$ \\
              & & $\rightarrow (\nseq{\texttt{i32}}{n} \rightarrow (\nseq{\alpha}{m}))$ \\
              & & $\rightarrow (\nseq{[d_{1}]\cdots[d_{n}]\alpha}{m})$ \\
    \kw{local}~$c$ & : & $\forall d\alpha.\rightarrow [d]\alpha \rightarrow [d]\alpha$ \\
    \StreamGroup{}
              &:& $\forall d x \bar{\alpha}^{(m)}\bar{\beta}^{(n)}\nseq{\gamma}{l}.$\\
              & & ~~~~~~~~~~ $\rightarrow ((x: \texttt{i32}) \rightarrow \nseq{\alpha}{m} \rightarrow \nseq{[x]\beta_i}{m} \rightarrow (\bar{\beta}^{(m)}, \nseq{[x]\gamma}{l}))$ \\
              & & ~~~~~~~~~~ $\rightarrow (\nseq{\alpha}{m})$\\
              & & ~~~~~~~~~~ $\rightarrow \nseq{[d]\beta_i}{n}$\\
              & & ~~~~~~~~~~ $\rightarrow (\nseq{\alpha}{m})$
\end{tabular}
  \caption{New constructs used to express kernel-level locality of reference optimisations.}
  \label{fig:lor-constructs}
\end{figure}

\section{Transposing for Coalesced Memory Access}
\label{sec:automatic-coalescing}

Ensuring coalesced accesses to global memory is critical for GPU
performance.  Several of the benchmarks discussed in
\cref{chap:empirical-validation}, such as FinPar's LocVolCalib,
Accelerate's $n$-body, and Rodinia's CFD, $k$-means, Myocyte, and
LavaMD, exhibit kernels in which one or several innermost dimensions
of the mapped arrays are processed sequentially inside the kernel.  In
the context of our flattening algorithm, this typically corresponds to
the case where rule \G{1} has been applied with $e$ being a SOAC.
This nested SOAC is transformed to a \StreamSeq{}; removing
parallelism, but retaining access pattern information.

A naive translation of a nested \lstinline{stream_seq} would lead to
consecutive threads accessing global memory with a stride equal to the
size of the inner (non-parallel) array dimensions, which may generate
one-order-of-magnitude slowdowns.
%
The Futhark compiler solves this by, intuitively, transposing the
non-parallel dimensions of the array innermost, and the same for the
result and all temporary arrays created inside the kernel.\footnote{
  The common case corresponds to directly mapped arrays, but we also
  perform simple index analysis to support common explicitly-indexed
  accesses.  }  This approach is guaranteed to resolve coalescing if
the sequential-dimension indices are invariant to the parallel array
dimensions. For example, consider the following expression:
\begin{lstlisting}[xleftmargin=0.5cm,numbers=none]
kernel (n) (\i -> stream_seq f (0) xss[i])
\end{lstlisting}
Assuming the inner reduction is implemented sequentially, the expression is optimized
by changing the representation of \texttt{xss} to be column major (the
default is row major), via transposition in memory, as follows:
\begin{lstlisting}[xleftmargin=0.5cm,numbers=none]
let xss' = manifest (1,0) xss
in kernel (n) (\i -> stream_seq f (0) xss'[i])
\end{lstlisting}

\noindent
The type of \texttt{xss'} is the same as that of \texttt{xss}.  The
representation of an array is recorded as a symbolic composition of
affine transformations, which needs to be examined to determine in
what way the source array should be transposed to achieve coalesced
accesses.

\section{Automatic Loop Tiling}
\label{sec:automatic-tiling}

The compiler also performs simple block \textit{tiling} of parallel
dimensions, which is driven by recognizing arrays that are used as
input to \lstinline{stream_seq} constructs \textit{and} are invariant
to one of the parallel dimensions.  For example, the code
\begin{lstlisting}[xleftmargin=0.5cm]
kernel (n) (\i -> stream_seq (f i) (ne) ps)
\end{lstlisting}

\noindent exhibits such an optimization opportunity for the streamed
array \lstinline{ps}, and is transformed into:

\begin{lstlisting}[xleftmargin=0.5cm]
kernel (n) (\i -> stream_group
                    (\q a (ps': [q]int) ->
                      let ps'' = local 0 ps'
                      in stream_seq (f i) (a) ps'')
                    (ne)
                    ps)
\end{lstlisting}

\noindent where \lstinline{ps''} is a fast memory\footnote{Called
  \textit{local memory} in OpenCL, and \textit{shared memory} in
  CUDA.} array created by collective copying (\kw{local}), and used
instead of \lstinline{ps'} in \lstinline{f}.  The size \lstinline{q}
is determined (at runtime) to be the group size used for this kernel,
such that the array \lstinline{ps''} will fit in the fast memory.
This example essentially illustrates the structure of the $n$-body
benchmark discussed in \cref{sec:empirical-evaluation}.

Futhark also supports two-dimensional tiling where two streamed arrays
are invariant to different parallel dimensions.  A matrix
multiplication that exhibits the pattern appears as follows.

\begin{lstlisting}[xleftmargin=0.5cm]
let yss' = rearrange (1,0) yss
in kernel (n,l) (\i j ->
     let xs = xss[i]
     let ys = yss'[j]
     in stream_seq f (0) xs ys)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f = \c acc xs' ys' ->}\\
\lstinline{      loop (acc) for i < c do}\\
\lstinline{        acc + xs'[i] * ys'[i]}
\end{minipage}
\vspace{1em}

The chunk function \lstinline{f} is computes the dot product of its
two input arrays and adds it to the accumulator.  The arrays
\lstinline{xs} and \lstinline{ys} are both invariant to at least one
dimension of the kernel, so they can be tiled as follows:

\begin{lstlisting}[xleftmargin=0.5cm]
let yss' = rearrange (1,0) yss
in kernel (n,l) (\i j ->
     let xs = xss[i]
     let ys = yss'[j]
     in stream_group g (0) xs ys
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{g = \c acc xs' ys' ->}\\
\lstinline{      let xs'' = local 1 xs'}\\
\lstinline{      let ys'' = local 0 ys'}\\
\lstinline{      in stream_seq f acc xs'' ys''}\\
\lstinline{f = }\textit{as before}
\end{minipage}
\vspace{1em}

The LavaMD benchmark from \cref{sec:rodinia} exhibits an interesting
tiling pattern, in which the to-be-tiled array is the result of an
indirect index computed by a function \texttt{f}, all nested inside of
a sequential loop.  A simplified reproduction:

\begin{lstlisting}[xleftmargin=0.5cm]
kernel (n,l) (\i j ->
  loop (outer_acc) = (...) for l < k do
    let j' = f l j
    let xs = xss[j']
    in stream_seq (h i) outer_acc xs)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f l j = let p = if 0 < l then ps[l,j] else j}\\
\lstinline{        in js[p]}\\
\lstinline{h i = }\textit{some function}
\end{minipage}
\vspace{1em}

Since the computation of the array \texttt{xss} is invariant to the
first dimension of the kernel, it can be tiled as follows:

\begin{lstlisting}[xleftmargin=0.5cm]
kernel (n,l) (\i j ->
  loop (outer_acc) = (...) for l < k do
    let j' = f l j
    let xs = xss[j']
    in stream_group g (0) outer_acc xs)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f l j = let p = if 0 < l then ps[l,j] else j}\\
\lstinline{        in js[p]}\\
\lstinline{g = \q acc xs' ->}\\
\lstinline{     let xs'' = local 1 xs'}\\
\lstinline{     in stream_seq (h i) acc xs''}\\
\lstinline{h i = }\textit{some function}
\end{minipage}
\vspace{1em}

Note that the tiling operation itself is not affected at all by the
convoluted computation of the index \lstinline{j'}.  All that we need
is the ability to compute which kernel dimensions \lstinline{j'} is
invariant to, which is reasonably simple in a pure language such as
Futhark.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
