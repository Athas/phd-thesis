\chapter{Optimising for Locality of Reference}
\label{chap:tiling}

While GPUs possess impressively fast memory, the ratio of computation
to memory speed is even more lopsided than on a CPU.  As a
consequence, accessing memory efficiently is of critical importance to
obtaining high performance.  Furthermore, while CPUs have multiple
layers of automatic caching to help mitigate the effect of the memory
wall, GPUs typically have only a single very small level-1 cache.
This chapter presents how two well-known locality-of-reference
optimisations can be applied to kernels produced by moderate
flattening.  The optimisations improves the memory access patterns of
Futhark programs by optimising both spatial and temporal locality of
reference.

\Cref{sec:automatic-coalescing} shows how arrays traversed in a kernel
can have their representation in memory modified to ensure that the
traversal is efficient.  This form of optimisation for spatial
locality of reference, which changes not just the \textit{code}, but
also the \textit{data}, is not usually used by compilers.
\Cref{sec:automatic-tiling} shows a technique for temporal locality of
reference, via loop tiling, that uses the local memory of the GPU as a
cache to minimise the amount of traffic to global memory.  While loop
tiling is an established optimisation technique, our approach can
handle even indirect accesses.  These are not supported by
conventional techniques for loop tiling, including polyhedral
approaches such as the ones discussed in
\cite{chatarasi2015polyhedral}.

To express the optimisations we have to extend the core Futhark IR,
yet again, with a few more operations.  Their types are shown on
\cref{fig:lor-constructs}, and their semantics below.

\begin{description}
\item[\kw{manifest} ($\nseq{c}{n}$) $x$] is used to force a particular
  representation of an $n$-dimensional array in memory.  The
  permutation ($c_{1}, \ldots, c_{n}$) indicates the order in
  which dimensions should occur.  Thus, for the two-dimensional case,\\
  \centerline{\mbox{\lstinline{manifest (0,1) xss}}}\\
  corresponds to a row-major
  (the default) copy of the array \lstinline{xss}, while\\
  \centerline{\mbox{\lstinline{manifest (1,0) xss}}}\\
  is a column-major copy.  Note that the type of the result of
  \kw{manifest} is the same as of the input array.  The only change is
  in how the array is located in memory, and thus the memory address
  computed when indexing an element at a given index.  As with
  \kw{rearrange}, the permutation must be a compile-time constant.
  The \kw{manifest} construct differs from \kw{rearrange} in that
  \kw{manifest} creates an actual array in memory, while
  \kw{rearrange} is merely a symbolic index space transformation.
\item[\kw{kernel} ($\nseq{d}{n}$)
  $(\Fn{\nseq{\tau}{m}}{\nseq{v}{2n}}{e})$] models an $n$-dimensional
  GPU kernel, loosely corresponding to a flattened form of $n$
  \kw{map}-nets, where nest number $i$ (counted from outside-in) has
  size $d_{i}$.  Semantically, the kernel function is executed for
  every thread, and passed $n$ \textit{global thread indices}, one for
  every dimension, and $n$ \textit{local thread indices}, again one
  for every dimension (see \cref{sec:kernel-thread-space}).  In many
  cases we will not be making use of the local thread indices, in
  which case we will simply put an underscore for the parameter names.
  The group size is not explicitly stated, but may be implicitly
  bounded through the use of \kw{local} (see below).

  If the kernel function returns $m$ values, then the \kw{kernel}
  construct as a whole returns $m$ arrays, each of of shape
  $[d_{1}]\cdots[d_{n}]$.  This corresponds to each thread returning a
  single value.
\item[\kw{local} $\nseq{c}{n}$ $x$] may only occur within
  \kw{kernel}s, and produces an array of shape $[c_{1}]\cdots[c_{n}]$
  where every thread contributes a value $x$.  Operationally,
  $\nseq{c}{n}$ corresponds to the group size of the kernel (and any
  kernel containing a \kw{local} is thus implicitly forced to have
  that group size), and the resulting array is located in local
  memory.  The subtleties of this construct is detailed further in
  \cref{sec:automatic-tiling}.
\end{description}

\begin{figure}[hbt]
  \begin{tabular}{lcl}
    \emph{op} & & \textrm{TySch}(\emph{op}) \\ \hline
    \kw{manifest} ($c_{1}, \ldots, c_{n}$) & : & $\forall\nseq{d}{n}\alpha.[d_{1}]\cdots[d_{n}]\alpha\rightarrow[d_1]\cdots[d_n]\alpha$ \\
    \kw{kernel} & : & $\forall \nseq{\alpha}{m}.\nseq{(d_{i}:\texttt{i32})}{n}$ \\
              & & $\rightarrow (\nseq{\texttt{i32}}{2n} \rightarrow (\nseq{\alpha}{m}))$ \\
              & & $\rightarrow (\nseq{[d_{1}]\cdots[d_{n}]\alpha}{m})$ \\
    \kw{local}~$c$ & : & $\forall \alpha. \nseq{(d_{i}: \texttt{i32})}{n} \rightarrow \alpha \rightarrow [d_{1}]\cdots[d_{n}]\alpha$ \\
\end{tabular}
\caption{The types of the language constructs used to express
  kernel-level locality of reference optimisations.}
  \label{fig:lor-constructs}
\end{figure}

\section{Transposing for Coalesced Memory Access}
\label{sec:automatic-coalescing}

Ensuring coalesced accesses to global memory is critical for GPU
performance.  Several of the benchmarks discussed in
\cref{chap:empirical-validation}, such as FinPar's LocVolCalib,
Accelerate's $n$-body, and Rodinia's CFD, $k$-means, Myocyte, and
LavaMD, exhibit kernels in which one or several innermost dimensions
of arrays are processed sequentially inside the kernel.  In the
context of the moderate flattening algorithm, this typically
corresponds to the case where rule \G{1} has been applied with $e$
being a SOAC.  For this discussion, we assume that the nested SOAC is
transformed to a \StreamSeq{}; removing parallelism, but retaining
access pattern information.

A naive translation of a nested \lstinline{stream_seq} would lead to
consecutive threads accessing global memory with a stride equal to the
size of the inner (non-parallel) array dimensions, which may generate
one-order-of-magnitude slowdowns on a GPU.
%
The Futhark compiler solves this by, intuitively, transposing the
non-parallel dimensions of the array outermost, and the same for the
result and all intermediate arrays created inside the kernel.  For
this thesis, we only discuss the first case, of arrays that are
sequentially iterated inside a kernel, as the others are not a
question of transforming an AST, but simply a question of how we
allocate arrays in the first place.  Our approach is guaranteed to
resolve coalescing if the sequential-dimension indices are invariant
to the parallel array dimensions. For example, consider the following
expression:
\begin{lstlisting}[numbers=none]
kernel (n) (\i _ -> stream_seq f (0) xss[i])
\end{lstlisting}
Assuming that
\lstinline{f} performs a sequential in-order traversal of its input,
the expression is optimized by changing the representation of
\texttt{xss} to be column major (the default is row major), via
transposition in memory, as follows:

\begin{lstlisting}[numbers=none]
let xss' = manifest (1,0) xss
in kernel (n) (\i _ -> stream_seq f (0) xss'[i])
\end{lstlisting}

The type of \texttt{xss'} is the same as that of \texttt{xss}.  The
difference between \texttt{xss} and \texttt{xss'} can be seen by
computing the flat index corresponding to an index expression.
Suppose \texttt{xss} has shape $[n][m]$, $i$ is the parallel (thread)
index, $j$ is the counter of a sequential loop, and $\textrm{flat}(x)$
is a function that returns a one-dimensional view of an array $x$;
then
\[
  \texttt{xss}[i][j] = \textrm{flat}(xss)[i\cdot{}m+j]
\]
versus
\[
  \texttt{xss'}[i][j] = \textrm{flat}(xss')[j\cdot{}m+i].
\]
It is clear that the latter gives rise to coalesced memory accesses,
while the former does not (assuming non-pathological values of $m$).

Concretely, the compiler inspects index expressions $x[\nseq{y}{n}]$
inside of kernels, where $x$ is a rank $m$ array that is created prior
to the kernel.  The inspection may yield a permutation $\nseq{c}{m}$
that is used to construct an array
\[
  x' = \kw{manifest}~(\nseq{c}{m})~x
\]
after which $x$ is replaced by $x'$ in the original index expression.
Two patterns are recognised by the inspection:

\begin{description}[style=nextline]
\item[Complete Index]

  The indices $\nseq{y}{n}$ contain at least some of the the global
  thread indices, and the innermost global thread index can be moved
  to the last position in the indices via the permutation
  $\nseq{c}{m}$.  We then copy the array with permutation
  $\nseq{c}{m}$.  An example is a kernel
\begin{lstlisting}[numbers=none]
kernel (n,m) (\i j _ _ -> xss[j][i] + 2)
\end{lstlisting}
which is transformed into
\begin{lstlisting}[numbers=none]
let xss' = manifest (1,0) xs
in kernel (n,m) (\i j _ _ -> xss'[j][i] + 2)
\end{lstlisting}

For this example, we could also have transposed the kernel dimensions
themselves, followed by transposing the result:
\begin{lstlisting}[numbers=none]
let r = kernel (m,n) (\j i _ _ -> xss'[j][i] + 2)
in rearrange (1,0) r
\end{lstlisting}
This saves on memory traffic, since \kw{rearrange} is a non-manifest
index space transformation.  However, this transformation affects the
memory layout of the result of the kernel, which may result in
non-coalesced accesses in later expressions.  As a result, we prefer
using \kw{manifest}, which has purely local effects.

Note that the pattern also applies to the case where the found
permutation is the identity permutation.  A useful optimisation is to
remove those \kw{manifest}s where it can be determined that the array
is already in the desired representation, either because of a
\kw{rearrange} that was already present in the source program, or
because the optimal representation is row-major, which is generally
the default for Futhark expressions.

\item[Incomplete Index]

  The indices $\nseq{y}{n}$ do not fully index the array ($n<m$), and
  at least one of the indices is variant to the thread indices.  An
  array index $y_{i}$ is variant to a thread index if there is a
  (conservatively estimated) data dependency from one of the thread
  indices passed to the kernel function to $y_{i}$.  In such cases, we
  use \kw{manifest} with the permutation
  $(n,\ldots,m-1,0,\ldots,n-1)$, corresponding to interchanging the
  sequentially traversed dimensions outermost.

  The (potentially wrong) assumption is that the dimensions that are
  not indexed will be traversed sequentially.  It remains future work
  to perform a more detailed analysis of how the resulting array slice
  is traversed.

  The requirement that at least one of $\nseq{y}{n}$ must be
  thread-variant is to avoid the pattern triggering on cases where an
  array is being indexed identically by all threads, such as
  \lstinline{xss[0]}.  Such indexes do not give rise to non-coalesced
  memory access.
\end{description}

Note that the transformation is on index expressions, not on arrays.
The same array \lstinline{xss} may be indexed in different ways within
the same kernel, and each distinct index expression may give rise to a
unique new \kw{manifest} array.  However, if the same array is
iterated in the same way in multiple places within the kernel, this
will still give rise only to a single array.

Our approach is not necessarily optimal---there are cases where
several distinct manifestations (as given by the rules above) all
solve the specific cases of uncoalesced access, but so might a
combined \textit{single} manifestation, with an appropriately selected
permutation.  This would require the algorithm to take a more
``global'' view, and has not yet been done.

\subsection{Viability of the Optimisation}

This optimisation requires additional runtime work and extra memory
space to perform transpositions before the kernel is invoked.  The
Futhark compiler presently always applies the optimisation whenever it
detects an array indexing of the appropriate form.  A relevant
question to ask is whether there are cases where this is not an
optimisation, but rather a \textit{pessimisation}.  The short answer:
in some cases, yes, but usually not.

First we remark that transpositions are quite fast on GPUs; running at
close to the speed of a memory copy.  Therefore, even in pathological
cases where where one of the dimensions is unit, or the inner
sequential loop is so short that the uncoalesced accesses have no
great impact, the cost of the transpositions is not catastrophic.

A greater concern is having to allocate extra memory to store the
transposed copies.  In the worst case, this may require temporarily
migrating the original arrays to CPU memory in order to fit the new
copies.  The CPU-GPU bandwidth is very low (current PCIe 4.0 x16 runs
at 31.51GiB/s in the best case), so this could lead to significant
slowdown.  In the future, we intend to investigate how to
``back-propagate'' the \kw{manifest} operations to the point at where
the array is originally created, and store it in the desired
representation in the first place.

\subsection{Indirect Accesses}

The coalescing transformation applies even in cases where indirect
accesses are involved, as it depends only on variance information.
For example, consider this variation on the earlier example, where an
array \texttt{is} of indices is used to index \texttt{xss}.

\begin{lstlisting}[numbers=none]
let (js: [n]i32) = ...
let (xss: [m][k]i32) = ...
in kernel (m) (\i _ ->
  loop acc = 0 for p < n do
    let j = js[p]
    let x = xss[i][j]
    in acc + x)
\end{lstlisting}

To the coalescing optimisation, this kernel contains two array
indices: \texttt{js[p]} and \texttt{xss[i,j]}.  The former is not
touched, since it does not involve any thread indices, and so does not
meet either of the rules.  However, the indexing expression
\texttt{xss[i,j]} \textit{does} match the rule for complete indexes,
since the global thread index \texttt{i} can be moved to the innermost
position via the permutation \texttt{(1,0)}.  Thus, the kernel is
transformed to the following, where all accesses are coalesced.

\begin{lstlisting}[numbers=none]
let (js: [n]i32) = ...
let (xss: [m][k]i32) = ...
let (xss': [m][k]i32) = manifest (1,0) xss
in kernel (m) (\i _ ->
  loop acc = 0 for p < n do
    let j = js[p]
    let x = xss'[i][j]
    in acc + x)
\end{lstlisting}

Note that the \texttt{p} index in \texttt{ps[p]} is invariant to the
kernel, and as such leads to effective caching (each thread in a warp
accesses the same \texttt{is[p]} in the same SIMD instruction).

\section{Automatic Loop Tiling}
\label{sec:automatic-tiling}

The Futhark compiler performs simple block \textit{tiling} in cases
where several threads are traversing the same array.  The tiling
algorithm transforms traversal to be over \textit{chunks} of the
input, where each chunk is collectively copied to local memory before
it is traversed.  The result is that the number of accesses to global
memory are reduced.

The algorithm is driven by recognizing arrays that are inputs to
\lstinline{stream_seq} constructs \textit{and} are invariant to one or
two of the kernel's innermost dimensions.  The tiling algorithm
proceeds by traversing the body of a \kw{kernel} and looking for an
appropriate \StreamSeq{} instance, which is then transformed using
either \textit{one-dimensional} or \textit{two-dimensional} tiling, as
detailed below.  In both cases, the \StreamSeq{} is transformed into
two nested \StreamSeq{}s, with the outer one iterating across chunks,
and the inner iterating across a single chunk.  We tile at most one
\StreamSeq{} for a kernel, as each instance of tiling may impose
constraints on the group size of the GPU kernel, and it is not clear
whether multiple instances of tiling would lead to conflicting
constraints.

In the following, we say that an array is variant to a parallel
dimension if there is no data dependency between the array and the
thread index corresponding to that dimension.  To simplify the
exposition, we are ignoring a significant amount of tedious complexity
with computing GPU group sizes and handling cases where the tiled
arrays have sizes that are not divisible with the optimal tile size.

\subsection{One-Dimensional Tiling}
\label{sec:one-dimensional-tiling}

One-dimensional tiling is performed when the input arrays of a
\StreamSeq{} are invariant to the same dimension.  We exploit a
\textit{stripmining property} of \StreamSeq{}, by which any
\StreamSeq{} can be transformed into two nested
\StreamSeq{}s.

For example, the kernel
\begin{lstlisting}
kernel (n) (\i li -> stream_seq (f i) (v) ps)
\end{lstlisting}
\noindent can have its \StreamSeq{} stripmined to obtain the following
\begin{lstlisting}
kernel (n)
  (\i li -> stream_seq
              (\q a (ps': [q]int) ->
                stream_seq (f i) (a) ps')
              (v)
              ps)
\end{lstlisting}

This kernel exhibits an optimization opportunity for the streamed
array \lstinline{ps}, as it is invariant to the first (and only)
dimension.  Concretely, the threads within a workgroup can
collectively copy the chunk \texttt{ps'} into local memory, which we
write as
\begin{lstlisting}
kernel (n)
  (\i li -> stream_seq
              (\q a (ps': [q]int) ->
                let x = ps'[li]
                let ps'' = local q x
                in stream_seq (f i) (a) ps'')
              (v)
              ps)
\end{lstlisting}
\noindent Recall that \texttt{li} is the local thread index.  Then,
\lstinline{ps''} is a local memory array created by collective copying
(\kw{local}), and used instead of \lstinline{ps'} in
\lstinline{f}. The size \lstinline{q} is then bounded (at runtime) by
the group size used for this kernel, such that the array
\lstinline{ps''} will fit in local memory.  For simplicity, we assume
that the size of array \texttt{ps} is divisible by some reasonable
group size.  The trick is that every thread within a workgroup reads a
distinct element from \texttt{ps'}, by using the local thread index.
The array \texttt{ps''} is visible by all threads within the
workgroup, so \kw{local} in effect constitutes intra-group
communication.

When tiling along a dimension $i$ ($1$ in the above example), it must
be ensured that each consecutive \lstinline{q} threads along that
dimension belong to the same GPU group.  In general, this is done by
forcing the group size to unit along all non-tiled dimensions,
although this is not necessary in the case of a unidimensional kernel.

One-dimensional tiling is used for the $n$-body benchmark discussed in
\cref{sec:rodinia}.  Note that the dimensionality of the kernel need
not be restricted to one for one-dimensional tiling to apply.
\Cref{sec:lavamd-tiling} contains an example of one-dimensional tiling
applied in a two-dimensional kernel.

\subsection{Two-Dimensional Tiling}
\label{sec:two-dimensional-tiling}

Futhark also supports two-dimensional tiling, where two streamed
arrays are invariant to different parallel dimensions.  A matrix
multiplication that exhibits the pattern appears as follows:

\begin{lstlisting}
let yss' = rearrange (1,0) yss
in kernel (n,l) (\i j li lj ->
     let xs = xss[i]
     let ys = yss'[j]
     in stream_seq f (0) xs ys)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f = \c acc xs' ys' ->}\\
\lstinline{      loop (acc) for i < c do}\\
\lstinline{        acc + xs'[i] * ys'[i]}
\end{minipage}
\vspace{1em}

The chunk function \lstinline{f} computes the dot product of its two
input arrays and adds it to the accumulator.  Prior to kernel
extraction, the \StreamSeq{} was in the form of a \kw{redomap}.  The
arrays \lstinline{xs} and \lstinline{ys} are both invariant to at
least one dimension of the kernel, so they can be tiled as follows:

\begin{lstlisting}
let yss' = rearrange (1,0) yss
in kernel (n,l) (\i j li lj ->
     let xs = xss[i]
     let ys = yss'[j]
     in stream_seq (g li lj) (0) xs ys
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{g = \li lj q acc xs' ys' ->}\\
\lstinline{      let x = xs'[lj]} \\
\lstinline{      let y = ys'[li]} \\
\lstinline{      let xss' = local q q x}\\
\lstinline{      let xs'' = xss'[li]}\\
\lstinline{      let yss' = local q q y}\\
\lstinline{      let yss_tr = rearrange (1,0) yss'}\\
\lstinline{      let ys''   = yss_tr[lj]}\\
\lstinline{      in stream_seq f acc xs'' ys''}\\
\lstinline{f = }\textit{as before}
\end{minipage}
\vspace{1em}

Operationally, the \lstinline{local} expressions creates a
two-dimensional array in local memory, which is then immediately
indexed with the local thread index along dimension $1$ (or $2$),
resulting in a one-dimensional array.  The core IR used in this thesis
does not support fixing any but the first dimension; instead we
transpose before indexing.

\subsection{A Complicated Instance of Tiling}
\label{sec:lavamd-tiling}

The LavaMD benchmark from \cref{sec:rodinia} exhibits an interesting
tiling pattern, in which the to-be-tiled array is the result of an
indirect index computed by a function \texttt{f}, all nested inside of
a sequential loop.  A simplified reproduction follows.

\begin{lstlisting}
kernel (n,m) (\i j li lj ->
  loop (outer_acc) = (...) for l < k do
    let i' = f l i
    let xs = xss[i']
    in stream_seq (h j) outer_acc xs)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f l i = let p = if 0 < l then ps[l,i] else j}\\
\lstinline{        in js[p]}\\
\lstinline{h j = }\textit{some function}
\end{minipage}
\vspace{1em}

Since the computation of the array \texttt{xss} is invariant to the
first dimension of the kernel (of size \texttt{n}), it can be tiled as
follows:

\begin{lstlisting}
kernel (n,m) (\i j li lj ->
  loop (outer_acc) = (...) for l < k do
    let i' = f l i
    let xs = xss[i']
    in stream_seq (g j lj) (0) outer_acc xs)
\end{lstlisting}
\begin{minipage}[t]{0.1\linewidth}
  \begin{flushright}
    where
  \end{flushright}
\end{minipage}
\begin{minipage}[t]{0.8\linewidth}
\lstinline{f l i = let p = if 0 < l then ps[l,i] else i}\\
\lstinline{        in js[p]}\\
\lstinline{g = \j lj q acc xs' ->}\\
\lstinline{        let x = xs'[lj]}\\
\lstinline{        let xss' = local 1 q x}\\
\lstinline{        let xs'' = xss'[0]}\\
\lstinline{        in stream_seq (h j) acc xs''}\\
\lstinline{h j = }\textit{some function}
\end{minipage}
\vspace{1em}

Note that the tiling operation itself is not affected at all by the
convoluted computation of the index \lstinline{j'}.  All that we need
is the ability to compute which kernel dimensions \lstinline{j'} is
invariant to, which is reasonably simple in a pure language such as
Futhark.

Again we assume that the second dimension (of size \texttt{m}) can be
divided by a reasonable group size (say, 256).  This assumption can be
removed by the addition of various tedious bounds checks and branches.
The use of \lstinline{local 1 q} forces that the two-dimensional group
size is always 1 along the outermost dimension, which maximises the
ratio of local-to-global memory accesses for array \lstinline{xs}.

\section{Related Work}

The intent of this chapter is not to argue that Futhark's scheme for
tiling and achieving coalesced memory accesses is superior to other
techniques.  Rather, we have merely shown that the moderate flattening
algorithm does not actively \textit{prevent} us from performing such
optimisations, as is the case for full flattening.

For example, in comparison to Futhark, imperative
analyses~\cite{InformalTiling,PolyPluto2} are superior at performing
all kinds of tiling, for example hexagonal time
tilling~\cite{HexaTiling} and achieving memory coalescing by
semantically transposing arrays on the fly (via tiling).
%
However, non-affine array indexes may restrict applicability: for
example indirect-array accesses would prevent them from optimising
memory coalescing for the OptionPricing benchmark (\cref{sec:finpar}),
where Futhark's simpler, transposition-based approach succeeds.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
